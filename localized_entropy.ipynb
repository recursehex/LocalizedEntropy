{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localized Entropy Notebook\n",
    "\n",
    "This notebook runs the end-to-end pipeline using the config in `configs/default.json`.\n",
    "Switch between synthetic and CTR data, tune the model, and toggle plots from the config file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d338d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from localized_entropy.config import (\n",
    "    load_and_resolve,\n",
    "    loss_label,\n",
    "    get_condition_label,\n",
    "    get_data_source,\n",
    "    resolve_loss_modes,\n",
    ")\n",
    "from localized_entropy.utils import init_device, set_seed\n",
    "from localized_entropy.data.pipeline import prepare_data\n",
    "from localized_entropy.training import evaluate, predict_probs\n",
    "from localized_entropy.analysis import (\n",
    "    print_pred_summary,\n",
    "    print_pred_stats_by_condition,\n",
    "    collect_le_stats_per_condition,\n",
    "    collect_logits,\n",
    "    bce_log_loss,\n",
    "    roc_auc_score,\n",
    "    pr_auc_score,\n",
    "    expected_calibration_error,\n",
    "    binary_classification_metrics,\n",
    "    per_condition_metrics,\n",
    "    summarize_per_ad_train_eval_rates,\n",
    ")\n",
    "from localized_entropy.plotting import (\n",
    "    plot_training_distributions,\n",
    "    plot_eval_log10p_hist,\n",
    "    plot_loss_curves,\n",
    "    plot_eval_predictions_by_condition,\n",
    "    plot_pred_to_train_rate,\n",
    "    plot_le_stats_per_condition,\n",
    "    plot_grad_sq_sums_by_condition,\n",
    "    plot_ctr_filter_stats,\n",
    "    plot_feature_distributions_by_condition,\n",
    "    plot_label_rates_by_condition,\n",
    "    build_eval_epoch_plotter,\n",
    "    build_eval_batch_plotter,\n",
    ")\n",
    "from localized_entropy.experiments import (\n",
    "    resolve_eval_bundle,\n",
    "    resolve_train_eval_bundle,\n",
    "    build_model,\n",
    "    train_single_loss,\n",
    "    build_seed_sequence,\n",
    "    run_repeated_loss_experiments,\n",
    ")\n",
    "from localized_entropy.compare import (\n",
    "    compare_bce_le_runs,\n",
    "    format_comparison_table,\n",
    "    format_bce_le_summary,\n",
    "    build_repeat_metrics_frame,\n",
    "    summarize_repeat_metrics,\n",
    "    build_wilcoxon_summary,\n",
    "    format_wilcoxon_summary,\n",
    "    build_per_condition_calibration_wilcoxon,\n",
    "    sort_per_condition_wilcoxon_frame,\n",
    ")\n",
    "from localized_entropy.outputs import build_output_paths, start_notebook_output_capture\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "torch.set_printoptions(precision=4)\n",
    "CONFIG_PATH = \"configs/default.json\"\n",
    "cfg = load_and_resolve(CONFIG_PATH)\n",
    "train_cfg = cfg[\"training\"]\n",
    "loss_modes = resolve_loss_modes(train_cfg.get(\"loss_mode\", \"localized_entropy\"))\n",
    "if not loss_modes:\n",
    "    raise ValueError(f\"Unsupported loss_mode: {train_cfg.get('loss_mode')}\")\n",
    "output_paths = {loss_mode: build_output_paths(cfg, loss_mode) for loss_mode in loss_modes}\n",
    "if \"notebook_output_capture\" in globals():\n",
    "    if hasattr(notebook_output_capture, \"stop\"):\n",
    "        notebook_output_capture.stop()\n",
    "notebook_output_capture = start_notebook_output_capture(output_paths)\n",
    "print(f\"Using experiment: {cfg['experiment'].get('name', cfg['experiment'].get('active', 'unknown'))}\")\n",
    "device, use_cuda, non_blocking = init_device()\n",
    "set_seed(cfg['project']['seed'], use_cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71762917",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bundle = prepare_data(cfg, device, use_cuda)\n",
    "splits = data_bundle.splits\n",
    "loaders = data_bundle.loaders\n",
    "plots_cfg = cfg['plots']\n",
    "train_cfg = cfg['training']\n",
    "data_source = get_data_source(cfg)\n",
    "condition_label = get_condition_label(cfg)\n",
    "loss_modes = resolve_loss_modes(train_cfg.get('loss_mode', 'localized_entropy'))\n",
    "if not loss_modes:\n",
    "    raise ValueError(f\"Unsupported loss_mode: {train_cfg.get('loss_mode')}\")\n",
    "train_both = len(loss_modes) > 1\n",
    "output_paths = {loss_mode: build_output_paths(cfg, loss_mode) for loss_mode in loss_modes}\n",
    "if train_both:\n",
    "    print(f\"Training loss modes: {', '.join(loss_modes)}\")\n",
    "raw_eval_value_range = plots_cfg.get('eval_pred_value_range', [-12, 0])\n",
    "if isinstance(raw_eval_value_range, (list, tuple)) and len(raw_eval_value_range) == 2:\n",
    "    eval_value_range = tuple(raw_eval_value_range)\n",
    "else:\n",
    "    print(\"[WARN] plots.eval_pred_value_range should be a 2-item list; using default (-12, 0).\")\n",
    "    eval_value_range = (-12, 0)\n",
    "# Configure evaluation target via configs/default.json -> evaluation.split.\n",
    "eval_cfg = cfg.get('evaluation', {})\n",
    "eval_split, eval_loader, eval_labels, eval_conds, eval_name = resolve_eval_bundle(\n",
    "    cfg, splits, loaders\n",
    ")\n",
    "eval_has_labels = eval_labels is not None\n",
    "train_eval_loader, train_eval_conds, train_eval_name = resolve_train_eval_bundle(\n",
    "    eval_split,\n",
    "    eval_loader,\n",
    "    eval_labels,\n",
    "    eval_conds,\n",
    "    eval_name,\n",
    "    loaders,\n",
    "    splits,\n",
    ")\n",
    "if cfg.get('logging', {}).get('print_eval_split', True):\n",
    "    print(f\"Evaluation split: {eval_name}\")\n",
    "    if train_eval_name != eval_name:\n",
    "        print(f\"Training eval split: {train_eval_name}\")\n",
    "if cfg.get('logging', {}).get('print_loader_note', True):\n",
    "    print(loaders.loader_note)\n",
    "if data_source == 'ctr' and cfg.get('ctr', {}).get('plot_filter_stats', False):\n",
    "    ctr_stats = data_bundle.plot_data.get('ctr_stats')\n",
    "    if ctr_stats:\n",
    "        plot_ctr_filter_stats(ctr_stats['stats_df'], ctr_stats['labels'], ctr_stats['filter_col'])\n",
    "if plots_cfg.get('data_before_training', False):\n",
    "    synth = data_bundle.plot_data.get('synthetic')\n",
    "    if synth:\n",
    "        plot_training_distributions(\n",
    "            synth['net_worth'],\n",
    "            synth['ages'],\n",
    "            synth['probs'],\n",
    "            synth['conds'],\n",
    "            synth['num_conditions'],\n",
    "        )\n",
    "    else:\n",
    "        if plots_cfg.get('ctr_data_distributions', True):\n",
    "            ctr_plot = data_bundle.plot_data.get('ctr_distributions')\n",
    "            if ctr_plot:\n",
    "                \n",
    "                print(\"DEBUG\")\n",
    "                print(ctr_plot['feature_names'])\n",
    "                print(ctr_plot['num_conditions'])\n",
    "                skip_features = {\"device_ip_count\", \"device_id_count\"}\n",
    "                feature_names = [\n",
    "                    name for name in ctr_plot['feature_names'] if name not in skip_features\n",
    "                ]\n",
    "                if feature_names:\n",
    "                    feature_indices = [\n",
    "                        i\n",
    "                        for i, name in enumerate(ctr_plot['feature_names'])\n",
    "                        if name not in skip_features\n",
    "                    ]\n",
    "                    filtered_xnum = ctr_plot['xnum'][:, feature_indices]\n",
    "                    log10_features = set(plots_cfg.get('ctr_log10_features', []))\n",
    "                    log10_features &= set(feature_names)\n",
    "                    plot_feature_distributions_by_condition(\n",
    "                        filtered_xnum,\n",
    "                        ctr_plot['conds'],\n",
    "                        feature_names,\n",
    "                        ctr_plot['num_conditions'],\n",
    "                        max_features=int(plots_cfg.get('ctr_max_features', 3)),\n",
    "                        log10_features=log10_features,\n",
    "                        density=bool(plots_cfg.get('ctr_use_density', False)),\n",
    "                    )\n",
    "                else:\n",
    "                    print('CTR distribution plots: all features filtered; skipping.')\n",
    "                if plots_cfg.get('ctr_label_rates', True):\n",
    "                    plot_label_rates_by_condition(\n",
    "                        ctr_plot['labels'],\n",
    "                        ctr_plot['conds'],\n",
    "                        ctr_plot['num_conditions'],\n",
    "                    )\n",
    "            else:\n",
    "                print('CTR plot_sample_size is disabled or empty; skipping CTR distributions.')\n",
    "        else:\n",
    "            print('CTR distribution plots are disabled in config.')\n",
    "else:\n",
    "    print('Skipping training data distribution plots before training.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics: input/label/condition stats\n",
    "from localized_entropy.analysis import (\n",
    "    print_condition_stats,\n",
    "    print_feature_stats,\n",
    "    print_label_stats,\n",
    ")\n",
    "\n",
    "print('Diagnostics: splits')\n",
    "print_feature_stats('Train features', splits.x_train)\n",
    "print_feature_stats('Eval features', splits.x_eval)\n",
    "if splits.x_test is not None:\n",
    "    print_feature_stats('Test features', splits.x_test)\n",
    "\n",
    "print_condition_stats('Train conds', splits.c_train, splits.num_conditions)\n",
    "print_condition_stats('Eval conds', splits.c_eval, splits.num_conditions)\n",
    "if splits.c_test is not None:\n",
    "    print_condition_stats('Test conds', splits.c_test, splits.num_conditions)\n",
    "\n",
    "print_label_stats('Train labels', splits.y_train, splits.c_train, splits.num_conditions)\n",
    "print_label_stats('Eval labels', splits.y_eval, splits.c_eval, splits.num_conditions)\n",
    "if splits.y_test is not None:\n",
    "    print_label_stats('Test labels', splits.y_test, splits.c_test, splits.num_conditions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d2b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "for loss_mode in loss_modes:\n",
    "    set_seed(cfg['project']['seed'], use_cuda)\n",
    "    model = build_model(cfg, splits, device)\n",
    "    models[loss_mode] = model\n",
    "    if len(loss_modes) == 1:\n",
    "        model\n",
    "    else:\n",
    "        print(f\"Model ({loss_label(loss_mode)}):\")\n",
    "        print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a27d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics: initial logits/prob stats (untrained)\n",
    "try:\n",
    "    batch = next(iter(train_eval_loader))\n",
    "except StopIteration:\n",
    "    batch = None\n",
    "if batch is None:\n",
    "    print(f\"{train_eval_name} loader empty; skipping init logits diagnostics.\")\n",
    "else:\n",
    "    x_b, x_cat_b, c_b, y_b, nw_b = batch\n",
    "    x_b = x_b.to(device, non_blocking=non_blocking)\n",
    "    x_cat_b = x_cat_b.to(device, non_blocking=non_blocking)\n",
    "    c_b = c_b.to(device, non_blocking=non_blocking)\n",
    "    for loss_mode, model in models.items():\n",
    "        label = loss_label(loss_mode)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x_b, x_cat_b, c_b)\n",
    "            probs = torch.sigmoid(logits)\n",
    "        logits_np = logits.detach().cpu().numpy().reshape(-1)\n",
    "        probs_np = probs.detach().cpu().numpy().reshape(-1)\n",
    "        print(\n",
    "            f\"{label} init batch logits: n={logits_np.size:,} min={logits_np.min():.6g} \"\n",
    "            f\"max={logits_np.max():.6g} mean={logits_np.mean():.6g} std={logits_np.std():.6g}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{label} init batch probs:  n={probs_np.size:,} min={probs_np.min():.6g} \"\n",
    "            f\"max={probs_np.max():.6g} mean={probs_np.mean():.6g} std={probs_np.std():.6g}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plots_cfg.get('eval_pred_by_condition', True):\n",
    "    for loss_mode, model in models.items():\n",
    "        loss_name = loss_label(loss_mode)\n",
    "        pretrain_eval_preds = predict_probs(\n",
    "            model,\n",
    "            train_eval_loader,\n",
    "            device,\n",
    "            non_blocking=non_blocking,\n",
    "        )\n",
    "        if pretrain_eval_preds.size > 0:\n",
    "            if train_eval_conds is None:\n",
    "                print(f\"{train_eval_name} conditions unavailable; skipping per-condition stats.\")\n",
    "            else:\n",
    "                print_pred_stats_by_condition(\n",
    "                    pretrain_eval_preds,\n",
    "                    train_eval_conds,\n",
    "                    splits.num_conditions,\n",
    "                    name=f\"Pre-Training {train_eval_name} ({loss_name})\",\n",
    "                )\n",
    "                plot_eval_predictions_by_condition(\n",
    "                    pretrain_eval_preds,\n",
    "                    train_eval_conds,\n",
    "                    splits.num_conditions,\n",
    "                    value_range=eval_value_range,\n",
    "                    title=(\n",
    "                        f\"Pre-Training {train_eval_name} Predictions by {condition_label} (\"\n",
    "                        f\"{loss_name})\"\n",
    "                    ),\n",
    "                )\n",
    "        else:\n",
    "            print(f\"{train_eval_name} set is empty after filtering; skipping pre-training plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce01bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_results = {}\n",
    "need_eval_logits = (\n",
    "    eval_has_labels\n",
    "    and (\n",
    "        train_both\n",
    "        or plots_cfg.get('le_stats', True)\n",
    "        or plots_cfg.get('print_le_stats_table', True)\n",
    "    )\n",
    ")\n",
    "plot_eval_epochs = plots_cfg.get('eval_pred_by_condition', True)\n",
    "eval_every_n_batches = int(train_cfg.get('eval_every_n_batches', 0) or 0)\n",
    "plot_eval_batches = plot_eval_epochs and eval_every_n_batches > 0\n",
    "track_eval_batch_losses = plots_cfg.get('loss_curves', True) and eval_every_n_batches > 0\n",
    "for loss_mode, model in models.items():\n",
    "    set_seed(cfg['project']['seed'], use_cuda)\n",
    "    loss_name = loss_label(loss_mode)\n",
    "    print(f\"[INFO] Training {loss_name} model\")\n",
    "    eval_callback = (\n",
    "        build_eval_epoch_plotter(\n",
    "            train_eval_name,\n",
    "            train_eval_conds,\n",
    "            splits.num_conditions,\n",
    "            eval_value_range,\n",
    "            condition_label,\n",
    "            loss_name,\n",
    "        )\n",
    "        if plot_eval_epochs else None\n",
    "    )\n",
    "    eval_batch_callback = (\n",
    "        build_eval_batch_plotter(\n",
    "            train_eval_name,\n",
    "            train_eval_conds,\n",
    "            splits.num_conditions,\n",
    "            eval_value_range,\n",
    "            condition_label,\n",
    "            loss_name,\n",
    "        )\n",
    "        if plot_eval_batches else None\n",
    "    )\n",
    "    run_results[loss_mode] = train_single_loss(\n",
    "        model=model,\n",
    "        loss_mode=loss_mode,\n",
    "        train_loader=loaders.train_loader,\n",
    "        train_eval_loader=train_eval_loader,\n",
    "        eval_loader=eval_loader,\n",
    "        device=device,\n",
    "        epochs=train_cfg['epochs'],\n",
    "        lr=train_cfg['lr'],\n",
    "        eval_has_labels=eval_has_labels,\n",
    "        non_blocking=non_blocking,\n",
    "        plot_eval_hist_epochs=plots_cfg.get('eval_hist_epochs', False),\n",
    "        eval_callback=eval_callback,\n",
    "        eval_every_n_batches=(\n",
    "            eval_every_n_batches if (plot_eval_batches or track_eval_batch_losses) else None\n",
    "        ),\n",
    "        eval_batch_callback=eval_batch_callback,\n",
    "        collect_eval_batch_losses=track_eval_batch_losses,\n",
    "        collect_grad_sq_sums=plots_cfg.get('grad_sq_by_condition', False),\n",
    "        collect_eval_logits=need_eval_logits,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db381ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plots_cfg.get('loss_curves', True):\n",
    "    for loss_mode, result in run_results.items():\n",
    "        output_path = output_paths[loss_mode][\"loss_curves\"]\n",
    "        plot_loss_curves(\n",
    "            result.train_losses,\n",
    "            result.eval_losses,\n",
    "            result.loss_label,\n",
    "            output_path=output_path,\n",
    "            eval_batch_losses=result.eval_batch_losses if eval_every_n_batches > 0 else None,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee164192",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cfg = cfg.get('evaluation', {})\n",
    "for loss_mode, result in run_results.items():\n",
    "    label = result.loss_label\n",
    "    eval_preds = result.eval_preds\n",
    "    eval_loss = result.eval_loss\n",
    "    model = result.model\n",
    "    display_name = eval_name if not train_both else f\"{eval_name} ({label})\"\n",
    "    print(f\"=== {label} model evaluation ===\")\n",
    "    if plots_cfg.get('print_eval_summary', True):\n",
    "        print_pred_summary(display_name, eval_preds, labels=eval_labels, conds=eval_conds)\n",
    "    if plots_cfg.get('eval_pred_hist', True):\n",
    "        plot_eval_log10p_hist(eval_preds.astype(np.float32), epoch=train_cfg['epochs'], name=display_name)\n",
    "    if plots_cfg.get('eval_pred_by_condition', True):\n",
    "        if eval_conds is None:\n",
    "            print(f\"{display_name} conditions unavailable; skipping eval predictions by condition.\")\n",
    "        else:\n",
    "            plot_eval_predictions_by_condition(\n",
    "                eval_preds,\n",
    "                eval_conds,\n",
    "                splits.num_conditions,\n",
    "                value_range=eval_value_range,\n",
    "                name=display_name,\n",
    "                output_path=output_paths[loss_mode][\"post_training_eval_predictions\"],\n",
    "                title=(\n",
    "                    f\"Post-Training {display_name} Predictions by {condition_label} (\"\n",
    "                    f\"{label})\"\n",
    "                ),\n",
    "            )\n",
    "            print(\"DEBUG: PREDICTIONS ARRAY\")\n",
    "            print(eval_preds[:100])\n",
    "    if eval_has_labels:\n",
    "        print(f\"Final {eval_name} {label}: {eval_loss:.10f}\")\n",
    "    else:\n",
    "        print(f\"Final {eval_name} {label}: n/a (labels unavailable)\")\n",
    "    if eval_has_labels:\n",
    "        bins = int(eval_cfg.get('ece_bins', 20))\n",
    "        min_count = int(eval_cfg.get('ece_min_count', 1))\n",
    "        small_prob_max_cfg = float(eval_cfg.get('small_prob_max', 0.01))\n",
    "        small_prob_quantile = float(eval_cfg.get('small_prob_quantile', 0.1))\n",
    "        total_bce = bce_log_loss(eval_preds, eval_labels)\n",
    "        total_ece, total_ece_table = expected_calibration_error(\n",
    "            eval_preds, eval_labels, bins=bins, min_count=min_count\n",
    "        )\n",
    "        small_threshold = small_prob_max_cfg\n",
    "        small_mask = eval_preds <= small_threshold\n",
    "        if not small_mask.any():\n",
    "            quantile_threshold = float(np.quantile(eval_preds, small_prob_quantile))\n",
    "            print(\n",
    "                f\"[INFO] No preds <= {small_threshold:g}; using {small_prob_quantile:.2f} quantile \"\n",
    "                f\"threshold {quantile_threshold:g} for small-prob calibration.\"\n",
    "            )\n",
    "            small_threshold = quantile_threshold\n",
    "            small_mask = eval_preds <= small_threshold\n",
    "        if small_mask.any():\n",
    "            total_ece_small, _ = expected_calibration_error(\n",
    "                eval_preds[small_mask],\n",
    "                eval_labels[small_mask],\n",
    "                bins=bins,\n",
    "                min_count=min_count,\n",
    "            )\n",
    "        else:\n",
    "            total_ece_small = float('nan')\n",
    "        print(f\"{label} Total BCE (log loss): {total_bce:.8f}\")\n",
    "        print(f\"{label} Total ECE: {total_ece:.8f}\")\n",
    "        print(f\"{label} Total ECE (small p<= {small_threshold:g}): {total_ece_small:.8f}\")\n",
    "        total_auc = roc_auc_score(eval_preds, eval_labels)\n",
    "        total_pr_auc = pr_auc_score(eval_preds, eval_labels)\n",
    "        print(f\"{label} Total ROC-AUC: {total_auc:.8f}\")\n",
    "        print(f\"{label} Total PR-AUC (AP): {total_pr_auc:.8f}\")\n",
    "        cls_metrics = binary_classification_metrics(eval_preds, eval_labels)\n",
    "        print(f\"{label} Total Accuracy@0.5: {cls_metrics['accuracy']:.8f}\")\n",
    "        print(f\"{label} Total F1@0.5: {cls_metrics['f1']:.8f}\")\n",
    "        if eval_cfg.get('print_calibration_table', False):\n",
    "            print(total_ece_table.to_string(index=False))\n",
    "        if eval_conds is None:\n",
    "            print(f\"[WARN] {display_name} conditions unavailable; skipping per-condition metrics.\")\n",
    "        else:\n",
    "            per_ad = per_condition_metrics(\n",
    "                eval_preds,\n",
    "                eval_labels,\n",
    "                eval_conds,\n",
    "                bins=bins,\n",
    "                min_count=min_count,\n",
    "                small_prob_max=small_threshold,\n",
    "            )\n",
    "            if eval_cfg.get('print_per_ad', True):\n",
    "                top_k = int(eval_cfg.get('per_ad_top_k', 10))\n",
    "                print(per_ad.head(top_k).to_string(index=False))\n",
    "        if not train_both:\n",
    "            for mode in train_cfg.get('eval_compare_losses', []):\n",
    "                if mode == loss_mode:\n",
    "                    continue\n",
    "                other_loss, _ = evaluate(\n",
    "                    model,\n",
    "                    eval_loader,\n",
    "                    device,\n",
    "                    loss_mode=mode,\n",
    "                    non_blocking=non_blocking,\n",
    "                )\n",
    "                other_label = loss_label(mode)\n",
    "                print(f\"Final {eval_name} {other_label}: {other_loss:.10f}\")\n",
    "    # Per-ad train click rates vs. eval prediction averages\n",
    "    plot_df = summarize_per_ad_train_eval_rates(\n",
    "        splits.y_train,\n",
    "        splits.c_train,\n",
    "        eval_preds,\n",
    "        eval_conds,\n",
    "        splits.num_conditions,\n",
    "        condition_label=condition_label,\n",
    "        eval_name=eval_name,\n",
    "        top_k=eval_cfg.get('per_ad_top_k', 10),\n",
    "    )\n",
    "    if plot_df is not None:\n",
    "        plot_pred_to_train_rate(\n",
    "            plot_df,\n",
    "            condition_label=condition_label,\n",
    "            eval_name=eval_name,\n",
    "            output_path=output_paths[loss_mode][\"pred_to_train_rate\"],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53859dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if loaders.test_loader is not None and eval_split != 'test':\n",
    "    for loss_mode, result in run_results.items():\n",
    "        label = result.loss_label\n",
    "        display_name = 'Test' if not train_both else f\"Test ({label})\"\n",
    "        test_preds = predict_probs(\n",
    "            result.model,\n",
    "            loaders.test_loader,\n",
    "            device,\n",
    "            non_blocking=non_blocking,\n",
    "        )\n",
    "        if test_preds.size > 0:\n",
    "            print_pred_summary(display_name, test_preds, labels=None, conds=splits.c_test)\n",
    "            if plots_cfg.get('eval_pred_hist', True):\n",
    "                plot_eval_log10p_hist(\n",
    "                    test_preds.astype(np.float32),\n",
    "                    epoch=train_cfg['epochs'],\n",
    "                    name=display_name,\n",
    "                )\n",
    "            if plots_cfg.get('eval_pred_by_condition', True):\n",
    "                if splits.c_test is None:\n",
    "                    print(f\"{display_name} conditions unavailable; skipping test predictions by condition.\")\n",
    "                else:\n",
    "                    plot_eval_predictions_by_condition(\n",
    "                        test_preds,\n",
    "                        splits.c_test,\n",
    "                        splits.num_conditions,\n",
    "                        value_range=eval_value_range,\n",
    "                        name=display_name,\n",
    "                        title=(\n",
    "                            f\"Post-Training {display_name} Predictions by {condition_label} (\"\n",
    "                            f\"{label})\"\n",
    "                        ),\n",
    "                    )\n",
    "        else:\n",
    "            print(f\"{display_name} set is empty after filtering; skipping test plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plots_cfg.get('data_after_training', False):\n",
    "    synth = data_bundle.plot_data.get('synthetic')\n",
    "    if synth:\n",
    "        plot_training_distributions(\n",
    "            synth['net_worth'],\n",
    "            synth['ages'],\n",
    "            synth['probs'],\n",
    "            synth['conds'],\n",
    "            synth['num_conditions'],\n",
    "        )\n",
    "    else:\n",
    "        if plots_cfg.get('ctr_data_distributions', True):\n",
    "            ctr_plot = data_bundle.plot_data.get('ctr_distributions')\n",
    "            if ctr_plot:\n",
    "                skip_features = {\"device_ip_count\", \"device_id_count\"}\n",
    "                feature_names = [\n",
    "                    name for name in ctr_plot['feature_names'] if name not in skip_features\n",
    "                ]\n",
    "                if feature_names:\n",
    "                    feature_indices = [\n",
    "                        i\n",
    "                        for i, name in enumerate(ctr_plot['feature_names'])\n",
    "                        if name not in skip_features\n",
    "                    ]\n",
    "                    filtered_xnum = ctr_plot['xnum'][:, feature_indices]\n",
    "                    log10_features = set(plots_cfg.get('ctr_log10_features', []))\n",
    "                    log10_features &= set(feature_names)\n",
    "                    plot_feature_distributions_by_condition(\n",
    "                        filtered_xnum,\n",
    "                        ctr_plot['conds'],\n",
    "                        feature_names,\n",
    "                        ctr_plot['num_conditions'],\n",
    "                        max_features=int(plots_cfg.get('ctr_max_features', 3)),\n",
    "                        log10_features=log10_features,\n",
    "                        density=bool(plots_cfg.get('ctr_use_density', False)),\n",
    "                    )\n",
    "                else:\n",
    "                    print('CTR distribution plots: all features filtered; skipping.')\n",
    "                if plots_cfg.get('ctr_label_rates', True):\n",
    "                    plot_label_rates_by_condition(\n",
    "                        ctr_plot['labels'],\n",
    "                        ctr_plot['conds'],\n",
    "                        ctr_plot['num_conditions'],\n",
    "                    )\n",
    "            else:\n",
    "                print('CTR plot_sample_size is disabled or empty; skipping CTR post-training plots.')\n",
    "        else:\n",
    "            print('CTR post-training plots are disabled in config.')\n",
    "else:\n",
    "    print('Post-training training data plots are disabled.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e419b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "need_le_stats = (\n",
    "    plots_cfg.get('le_stats', True)\n",
    "    or plots_cfg.get('print_le_stats_table', True)\n",
    "    or train_both\n",
    ")\n",
    "if need_le_stats:\n",
    "    if not eval_has_labels:\n",
    "        print(f\"{eval_name} labels unavailable; skipping localized entropy stats.\")\n",
    "    elif eval_conds is None:\n",
    "        print(f\"{eval_name} conditions unavailable; skipping localized entropy stats.\")\n",
    "    else:\n",
    "        for loss_mode, result in run_results.items():\n",
    "            label = result.loss_label\n",
    "            z_all = result.eval_logits\n",
    "            y_all = result.eval_targets\n",
    "            c_all = result.eval_conds\n",
    "            if z_all is None or y_all is None or c_all is None:\n",
    "                z_all, y_all, c_all = collect_logits(\n",
    "                    result.model, eval_loader, device, non_blocking=non_blocking\n",
    "                )\n",
    "                result.eval_logits = z_all\n",
    "                result.eval_targets = y_all\n",
    "                result.eval_conds = c_all\n",
    "            le_stats = collect_le_stats_per_condition(z_all, y_all, c_all, eps=1e-12)\n",
    "            if plots_cfg.get('print_le_stats_table', True):\n",
    "                print(f\"LE stats table ({label})\")\n",
    "                print('cond\tnum\tden\tavg_p\t#y=1\t#y=0\tratio')\n",
    "                for cond in sorted(le_stats.keys()):\n",
    "                    s = le_stats[cond]\n",
    "                    print(\n",
    "                        f\"{cond}\t{s['Numerator']:.6g}\t{s['Denominator']:.6g}\t\"\n",
    "                        f\"{s['Average prediction for denominator']:.6g}\t\"\n",
    "                        f\"{s['Number of samples with label 1']}\t\"\n",
    "                        f\"{s['Number of samples with label 0']}\t\"\n",
    "                        f\"{s['Numerator/denominator']:.6g}\"\n",
    "                    )\n",
    "            if plots_cfg.get('le_stats', True):\n",
    "                plot_le_stats_per_condition(\n",
    "                    le_stats,\n",
    "                    title=f\"Localized Entropy terms per condition - {eval_name} set ({label})\",\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91fde3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_cfg = cfg.get('comparison', {})\n",
    "if train_both and compare_cfg.get('enabled', True):\n",
    "    if not eval_has_labels or eval_conds is None:\n",
    "        print(f\"[WARN] {eval_name} labels/conditions unavailable; skipping BCE vs LE comparison.\")\n",
    "    else:\n",
    "        bce_result = run_results.get('bce')\n",
    "        le_result = run_results.get('localized_entropy')\n",
    "        if bce_result is None or le_result is None:\n",
    "            print(\"[WARN] Missing BCE or LE results; skipping comparison.\")\n",
    "        else:\n",
    "            comparison = compare_bce_le_runs(\n",
    "                bce_result,\n",
    "                le_result,\n",
    "                eval_labels,\n",
    "                eval_conds,\n",
    "                condition_label=condition_label,\n",
    "                sort_by=str(compare_cfg.get('sort_by', 'count')),\n",
    "            )\n",
    "            if compare_cfg.get('print_table', True):\n",
    "                table_columns = [\n",
    "                    condition_label,\n",
    "                    'count',\n",
    "                    'base_rate',\n",
    "                    'bce_pred_mean',\n",
    "                    'le_pred_mean',\n",
    "                    'bce_calibration',\n",
    "                    'le_calibration',\n",
    "                    'delta_calibration',\n",
    "                    'bce_le_ratio',\n",
    "                    'le_le_ratio',\n",
    "                    'delta_le_ratio',\n",
    "                ]\n",
    "                title = f\"{eval_name}: per-{condition_label} BCE vs LE comparison\"\n",
    "                print(title)\n",
    "                print(format_comparison_table(\n",
    "                    comparison,\n",
    "                    table_columns,\n",
    "                    top_k=int(compare_cfg.get('top_k', 20)),\n",
    "                ))\n",
    "                print('')\n",
    "                eval_cfg = cfg.get('evaluation', {})\n",
    "                print(format_bce_le_summary(\n",
    "                    bce_result,\n",
    "                    le_result,\n",
    "                    eval_labels,\n",
    "                    ece_bins=int(eval_cfg.get('ece_bins', 20)),\n",
    "                    ece_min_count=int(eval_cfg.get('ece_min_count', 1)),\n",
    "                ))\n",
    "\n",
    "grad_plot_enabled = plots_cfg.get('grad_sq_by_condition', False)\n",
    "if grad_plot_enabled:\n",
    "    if not train_both:\n",
    "        print('[WARN] Grad sum plot requires BCE and LE runs; skipping.')\n",
    "    else:\n",
    "        bce_result = run_results.get('bce')\n",
    "        le_result = run_results.get('localized_entropy')\n",
    "        if bce_result is None or le_result is None:\n",
    "            print('[WARN] Missing BCE or LE results; skipping grad sum plot.')\n",
    "        else:\n",
    "            bce_grads = bce_result.grad_sq_sum_per_condition\n",
    "            le_grads = le_result.grad_sq_sum_per_condition\n",
    "            if bce_grads is None or le_grads is None:\n",
    "                print('[WARN] Grad sums not collected; enable plots.grad_sq_by_condition to track.')\n",
    "            else:\n",
    "                plot_grad_sq_sums_by_condition(\n",
    "                    bce_grads,\n",
    "                    le_grads,\n",
    "                    condition_label=condition_label,\n",
    "                    title=(\n",
    "                        f\"Training gradient sum of squares by {condition_label} (BCE vs LE)\"\n",
    "                    ),\n",
    "                    top_k=int(plots_cfg.get('grad_sq_top_k', 0) or 0),\n",
    "                    log10=bool(plots_cfg.get('grad_sq_log10', True)),\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a426193",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_cfg = cfg.get('repeats', {})\n",
    "repeat_enabled = bool(repeat_cfg.get('enabled', False))\n",
    "repeat_runs = int(repeat_cfg.get('num_runs', 1) or 0)\n",
    "repeat_seed_stride = int(repeat_cfg.get('seed_stride', 1) or 1)\n",
    "include_base_run = bool(repeat_cfg.get('include_base_run', True))\n",
    "if repeat_enabled:\n",
    "    if repeat_runs < 1:\n",
    "        print('[WARN] repeats.enabled is true but repeats.num_runs < 1; skipping repeats.')\n",
    "    elif not train_both:\n",
    "        print('[WARN] Repeat runs require training both loss modes; skipping.')\n",
    "    elif not eval_has_labels:\n",
    "        print(f\"[WARN] {eval_name} labels unavailable; skipping repeat stats.\")\n",
    "    else:\n",
    "        base_seed = int(cfg['project']['seed'])\n",
    "        seeds = build_seed_sequence(base_seed, repeat_runs, repeat_seed_stride)\n",
    "        repeat_results = {loss_mode: [] for loss_mode in loss_modes}\n",
    "        run_seeds = []\n",
    "        start_idx = 0\n",
    "        if include_base_run:\n",
    "            missing = [mode for mode in loss_modes if mode not in run_results]\n",
    "            if missing:\n",
    "                print(f\"[WARN] Missing base run results for {missing}; running all repeats from scratch.\")\n",
    "            else:\n",
    "                for loss_mode in loss_modes:\n",
    "                    repeat_results[loss_mode].append(run_results[loss_mode])\n",
    "                run_seeds.append(seeds[0] if seeds else base_seed)\n",
    "                start_idx = 1\n",
    "        extra_seeds = seeds[start_idx:]\n",
    "        if extra_seeds:\n",
    "            print(f\"[INFO] Running {len(extra_seeds)} repeated runs for significance testing.\")\n",
    "            extra_results = run_repeated_loss_experiments(\n",
    "                cfg=cfg,\n",
    "                loss_modes=loss_modes,\n",
    "                splits=splits,\n",
    "                loaders=loaders,\n",
    "                train_eval_loader=train_eval_loader,\n",
    "                eval_loader=eval_loader,\n",
    "                device=device,\n",
    "                use_cuda=use_cuda,\n",
    "                eval_has_labels=eval_has_labels,\n",
    "                seeds=extra_seeds,\n",
    "                non_blocking=non_blocking,\n",
    "                collect_eval_logits=False,\n",
    "            )\n",
    "            for loss_mode, runs in extra_results.items():\n",
    "                repeat_results[loss_mode].extend(runs)\n",
    "            run_seeds.extend(extra_seeds)\n",
    "        bce_runs = repeat_results.get('bce', [])\n",
    "        le_runs = repeat_results.get('localized_entropy', [])\n",
    "        if len(bce_runs) != len(le_runs):\n",
    "            print('[WARN] Repeat run counts do not match between BCE and LE; skipping Wilcoxon test.')\n",
    "        elif not bce_runs:\n",
    "            print('[WARN] No repeated runs available for significance testing.')\n",
    "        else:\n",
    "            eval_cfg = cfg.get('evaluation', {})\n",
    "            run_values = run_seeds if len(run_seeds) == len(bce_runs) else None\n",
    "            bce_metrics = build_repeat_metrics_frame(\n",
    "                bce_runs,\n",
    "                eval_labels,\n",
    "                ece_bins=int(eval_cfg.get('ece_bins', 20)),\n",
    "                ece_min_count=int(eval_cfg.get('ece_min_count', 1)),\n",
    "                threshold=0.5,\n",
    "                small_prob_max=float(eval_cfg.get('small_prob_max', 0.01)),\n",
    "                small_prob_quantile=float(eval_cfg.get('small_prob_quantile', 0.1)),\n",
    "                run_label='seed',\n",
    "                run_values=run_values,\n",
    "            )\n",
    "            le_metrics = build_repeat_metrics_frame(\n",
    "                le_runs,\n",
    "                eval_labels,\n",
    "                ece_bins=int(eval_cfg.get('ece_bins', 20)),\n",
    "                ece_min_count=int(eval_cfg.get('ece_min_count', 1)),\n",
    "                threshold=0.5,\n",
    "                small_prob_max=float(eval_cfg.get('small_prob_max', 0.01)),\n",
    "                small_prob_quantile=float(eval_cfg.get('small_prob_quantile', 0.1)),\n",
    "                run_label='seed',\n",
    "                run_values=run_values,\n",
    "            )\n",
    "            print('Repeated-run metric summary (BCE):')\n",
    "            print(format_comparison_table(\n",
    "                summarize_repeat_metrics(bce_metrics),\n",
    "                ['metric', 'n', 'mean', 'std', 'min', 'max'],\n",
    "                top_k=10,\n",
    "            ))\n",
    "            print('')\n",
    "            print('Repeated-run metric summary (LE):')\n",
    "            print(format_comparison_table(\n",
    "                summarize_repeat_metrics(le_metrics),\n",
    "                ['metric', 'n', 'mean', 'std', 'min', 'max'],\n",
    "                top_k=10,\n",
    "            ))\n",
    "            print('')\n",
    "            if len(bce_runs) < 2:\n",
    "                print('[WARN] Need at least 2 repeats for a Wilcoxon test; skipping.')\n",
    "            else:\n",
    "                wilcoxon_summary = build_wilcoxon_summary(\n",
    "                    bce_metrics,\n",
    "                    le_metrics,\n",
    "                    zero_method=str(repeat_cfg.get('wilcoxon_zero_method', 'wilcox')),\n",
    "                    alternative=str(repeat_cfg.get('wilcoxon_alternative', 'two-sided')),\n",
    "                )\n",
    "                print('Wilcoxon signed-rank test (delta > 0 favors LE):')\n",
    "                print(format_wilcoxon_summary(wilcoxon_summary))\n",
    "                if eval_conds is None:\n",
    "                    print('[WARN] Eval conditions unavailable; skipping per-condition calibration test.')\n",
    "                else:\n",
    "                    per_cond_min_count = int(repeat_cfg.get('per_condition_min_count', 1))\n",
    "                    per_cond_sort_by = str(repeat_cfg.get('per_condition_sort_by', 'p_value'))\n",
    "                    per_cond_top_k = int(repeat_cfg.get('per_condition_top_k', 20))\n",
    "                    per_cond_summary = build_per_condition_calibration_wilcoxon(\n",
    "                        bce_runs,\n",
    "                        le_runs,\n",
    "                        eval_labels,\n",
    "                        eval_conds,\n",
    "                        zero_method=str(repeat_cfg.get('wilcoxon_zero_method', 'wilcox')),\n",
    "                        alternative=str(repeat_cfg.get('wilcoxon_alternative', 'two-sided')),\n",
    "                        min_count=per_cond_min_count,\n",
    "                    )\n",
    "                    if per_cond_summary is None or len(per_cond_summary) == 0:\n",
    "                        print('[WARN] Per-condition calibration test returned no rows.')\n",
    "                    else:\n",
    "                        per_cond_summary = per_cond_summary.rename(columns={'condition': condition_label})\n",
    "                        per_cond_summary = sort_per_condition_wilcoxon_frame(per_cond_summary, per_cond_sort_by)\n",
    "                        print(f'Per-{condition_label} calibration Wilcoxon (abs pred_mean - base_rate):')\n",
    "                        print(format_comparison_table(\n",
    "                            per_cond_summary,\n",
    "                            [condition_label, 'count', 'base_rate', 'bce_gap', 'le_gap', 'delta_mean', 'p_value'],\n",
    "                            top_k=per_cond_top_k,\n",
    "                        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0331edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"notebook_output_capture\" in globals():\n",
    "    if hasattr(notebook_output_capture, \"stop\"):\n",
    "        notebook_output_capture.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
