{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Localized Entropy Notebook\n",
        "\n",
        "This notebook runs the end-to-end pipeline using the config in `configs/default.json`.\n",
        "Switch between synthetic and CTR data, tune the model, and toggle plots from the config file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d338d8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Set up imports, config, output capture, and device/seed.\"\"\"\n",
        "%matplotlib inline\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from localized_entropy.config import (\n",
        "    load_and_resolve,\n",
        "    loss_label,\n",
        "    get_condition_label,\n",
        "    get_data_source,\n",
        "    resolve_loss_modes,\n",
        ")\n",
        "from localized_entropy.utils import init_device, set_seed\n",
        "from localized_entropy.data.pipeline import prepare_data\n",
        "from localized_entropy.training import evaluate, predict_probs\n",
        "from localized_entropy.analysis import (\n",
        "    print_pred_summary,\n",
        "    print_pred_stats_by_condition,\n",
        "    collect_le_stats_per_condition,\n",
        "    collect_logits,\n",
        "    bce_log_loss,\n",
        "    roc_auc_score,\n",
        "    pr_auc_score,\n",
        "    expected_calibration_error,\n",
        "    binary_classification_metrics,\n",
        "    per_condition_metrics,\n",
        "    per_condition_mean,\n",
        "    per_condition_calibration,\n",
        "    per_condition_calibration_from_base_rates,\n",
        "    summarize_per_ad_train_eval_rates,\n",
        ")\n",
        "from localized_entropy.plotting import (\n",
        "    plot_training_distributions,\n",
        "    plot_eval_log10p_hist,\n",
        "    plot_loss_curves,\n",
        "    plot_eval_predictions_by_condition,\n",
        "    plot_calibration_ratio_by_condition,\n",
        "    plot_pred_to_train_rate,\n",
        "    plot_le_stats_per_condition,\n",
        "    plot_grad_sq_sums_by_condition,\n",
        "    plot_ctr_filter_stats,\n",
        "    plot_feature_distributions_by_condition,\n",
        "    plot_label_rates_by_condition,\n",
        "    build_eval_epoch_plotter,\n",
        "    build_eval_batch_plotter,\n",
        ")\n",
        "from localized_entropy.experiments import (\n",
        "    resolve_eval_bundle,\n",
        "    resolve_train_eval_bundle,\n",
        "    build_loss_loaders,\n",
        "    select_eval_loader,\n",
        "    build_model,\n",
        "    train_single_loss,\n",
        "    build_seed_sequence,\n",
        "    run_repeated_loss_experiments,\n",
        ")\n",
        "from localized_entropy.compare import (\n",
        "    compare_bce_le_runs,\n",
        "    summarize_model_metrics,\n",
        "    format_comparison_table,\n",
        "    format_bce_le_summary,\n",
        "    build_repeat_metrics_frame,\n",
        "    summarize_repeat_metrics,\n",
        "    build_wilcoxon_summary,\n",
        "    format_wilcoxon_summary,\n",
        "    build_per_condition_calibration_wilcoxon,\n",
        "    sort_per_condition_wilcoxon_frame,\n",
        ")\n",
        "from localized_entropy.outputs import build_output_paths, start_notebook_output_capture\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "torch.set_printoptions(precision=4)\n",
        "# Load config and resolve experiment overrides\n",
        "CONFIG_PATH = \"configs/default.json\"\n",
        "cfg = load_and_resolve(CONFIG_PATH)\n",
        "train_cfg = cfg[\"training\"]\n",
        "loss_modes = resolve_loss_modes(train_cfg.get(\"loss_mode\", \"localized_entropy\"))\n",
        "if not loss_modes:\n",
        "    raise ValueError(f\"Unsupported loss_mode: {train_cfg.get('loss_mode')}\")\n",
        "# Start output capture for each loss mode\n",
        "output_paths = {loss_mode: build_output_paths(cfg, loss_mode) for loss_mode in loss_modes}\n",
        "if \"notebook_output_capture\" in globals():\n",
        "    if hasattr(notebook_output_capture, \"stop\"):\n",
        "        notebook_output_capture.stop()\n",
        "notebook_output_capture = start_notebook_output_capture(output_paths)\n",
        "print(f\"Using experiment: {cfg['experiment'].get('name', cfg['experiment'].get('active', 'unknown'))}\")\n",
        "# Initialize device and RNG seeds\n",
        "device_cfg = cfg.get(\"device\", {})\n",
        "use_mps_flag = bool(device_cfg.get(\"use_mps\", True))\n",
        "device, use_cuda, use_mps, non_blocking = init_device(use_mps=use_mps_flag)\n",
        "cpu_float64 = device.type == \"cpu\" and not use_mps_flag\n",
        "model_dtype = torch.float64 if cpu_float64 else torch.float32\n",
        "set_seed(cfg['project']['seed'], use_cuda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71762917",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Prepare data loaders, resolve eval split, and plot pre-training data distributions.\"\"\"\n",
        "data_bundle = prepare_data(cfg, device, use_cuda, use_mps)\n",
        "splits = data_bundle.splits\n",
        "loaders = data_bundle.loaders\n",
        "plots_cfg = cfg['plots']\n",
        "train_cfg = cfg['training']\n",
        "data_source = get_data_source(cfg)\n",
        "condition_label = get_condition_label(cfg)\n",
        "loss_modes = resolve_loss_modes(train_cfg.get('loss_mode', 'localized_entropy'))\n",
        "if not loss_modes:\n",
        "    raise ValueError(f\"Unsupported loss_mode: {train_cfg.get('loss_mode')}\")\n",
        "train_multi = len(loss_modes) > 1\n",
        "train_bce_le = (\"bce\" in loss_modes and \"localized_entropy\" in loss_modes)\n",
        "output_paths = {loss_mode: build_output_paths(cfg, loss_mode) for loss_mode in loss_modes}\n",
        "if train_multi:\n",
        "    print(f\"Training loss modes: {', '.join(loss_modes)}\")\n",
        "raw_eval_value_range = plots_cfg.get('eval_pred_value_range', [-12, 0])\n",
        "if isinstance(raw_eval_value_range, (list, tuple)) and len(raw_eval_value_range) == 2:\n",
        "    eval_value_range = tuple(raw_eval_value_range)\n",
        "else:\n",
        "    print(\"[WARN] plots.eval_pred_value_range should be a 2-item list; using default (-12, 0).\")\n",
        "    eval_value_range = (-12, 0)\n",
        "# Configure evaluation target via configs/default.json -> evaluation.split.\n",
        "# Keep a train-time eval split with labels/conds for plots even if eval/test lacks labels.\n",
        "# Resolve evaluation split + training eval split (for labeled diagnostics)\n",
        "eval_cfg = cfg.get('evaluation', {})\n",
        "eval_split, eval_loader, eval_labels, eval_conds, eval_name = resolve_eval_bundle(\n",
        "    cfg, splits, loaders\n",
        ")\n",
        "eval_has_labels = eval_labels is not None\n",
        "train_eval_loader, train_eval_conds, train_eval_name = resolve_train_eval_bundle(\n",
        "    eval_split,\n",
        "    eval_loader,\n",
        "    eval_labels,\n",
        "    eval_conds,\n",
        "    eval_name,\n",
        "    loaders,\n",
        "    splits,\n",
        ")\n",
        "le_base_rates_train = None\n",
        "le_base_rates_train_eval = None\n",
        "le_base_rates_eval = None\n",
        "use_true_le_base_rates = False\n",
        "# Optional: use true synthetic base rates for LE denominator\n",
        "if data_source == 'synthetic' and cfg.get('synthetic', {}).get('use_true_base_rates_for_le', False):\n",
        "    base_rates_train = per_condition_mean(splits.p_train, splits.c_train, splits.num_conditions)\n",
        "    base_rates_eval = per_condition_mean(splits.p_eval, splits.c_eval, splits.num_conditions)\n",
        "    if base_rates_train is not None:\n",
        "        use_true_le_base_rates = True\n",
        "        le_base_rates_train = base_rates_train\n",
        "        le_base_rates_train_eval = base_rates_train if train_eval_name == 'Train' else base_rates_eval\n",
        "        if eval_name == 'Train':\n",
        "            le_base_rates_eval = base_rates_train\n",
        "        elif eval_name == 'Eval':\n",
        "            le_base_rates_eval = base_rates_eval\n",
        "        else:\n",
        "            le_base_rates_eval = None\n",
        "        print('[INFO] Using true per-condition base rates for LE denominator (synthetic).')\n",
        "# Logging: split selection and loader notes\n",
        "if cfg.get('logging', {}).get('print_eval_split', True):\n",
        "    print(f\"Evaluation split: {eval_name}\")\n",
        "    if train_eval_name != eval_name:\n",
        "        print(f\"Training eval split: {train_eval_name}\")\n",
        "if cfg.get('logging', {}).get('print_loader_note', True):\n",
        "    print(loaders.loader_note)\n",
        "# Optional CTR filter stats plot\n",
        "if data_source == 'ctr' and cfg.get('ctr', {}).get('plot_filter_stats', False):\n",
        "    ctr_stats = data_bundle.plot_data.get('ctr_stats')\n",
        "    if ctr_stats:\n",
        "        plot_ctr_filter_stats(ctr_stats['stats_df'], ctr_stats['labels'], ctr_stats['filter_col'])\n",
        "# Optional pre-training data distribution plots\n",
        "if plots_cfg.get('data_before_training', False):\n",
        "    synth = data_bundle.plot_data.get('synthetic')\n",
        "    if synth:\n",
        "        plot_training_distributions(\n",
        "            synth['net_worth'],\n",
        "            synth['ages'],\n",
        "            synth['probs'],\n",
        "            synth['conds'],\n",
        "            synth['num_conditions'],\n",
        "        )\n",
        "    else:\n",
        "        if plots_cfg.get('ctr_data_distributions', True):\n",
        "            ctr_plot = data_bundle.plot_data.get('ctr_distributions')\n",
        "            if ctr_plot:\n",
        "                \n",
        "                print(\"DEBUG\")\n",
        "                print(ctr_plot['feature_names'])\n",
        "                print(ctr_plot['num_conditions'])\n",
        "                # Skip high-cardinality device count features to keep plots readable.\n",
        "                skip_features = {\"device_ip_count\", \"device_id_count\"}\n",
        "                feature_names = [\n",
        "                    name for name in ctr_plot['feature_names'] if name not in skip_features\n",
        "                ]\n",
        "                if feature_names:\n",
        "                    feature_indices = [\n",
        "                        i\n",
        "                        for i, name in enumerate(ctr_plot['feature_names'])\n",
        "                        if name not in skip_features\n",
        "                    ]\n",
        "                    filtered_xnum = ctr_plot['xnum'][:, feature_indices]\n",
        "                    # Apply log scaling only to configured numeric features.\n",
        "                    log10_features = set(plots_cfg.get('ctr_log10_features', []))\n",
        "                    log10_features &= set(feature_names)\n",
        "                    plot_feature_distributions_by_condition(\n",
        "                        filtered_xnum,\n",
        "                        ctr_plot['conds'],\n",
        "                        feature_names,\n",
        "                        ctr_plot['num_conditions'],\n",
        "                        max_features=int(plots_cfg.get('ctr_max_features', 3)),\n",
        "                        log10_features=log10_features,\n",
        "                        density=bool(plots_cfg.get('ctr_use_density', False)),\n",
        "                    )\n",
        "                else:\n",
        "                    print('CTR distribution plots: all features filtered; skipping.')\n",
        "                if plots_cfg.get('ctr_label_rates', True):\n",
        "                    plot_label_rates_by_condition(\n",
        "                        ctr_plot['labels'],\n",
        "                        ctr_plot['conds'],\n",
        "                        ctr_plot['num_conditions'],\n",
        "                    )\n",
        "            else:\n",
        "                print('CTR plot_sample_size is disabled or empty; skipping CTR distributions.')\n",
        "        else:\n",
        "            print('CTR distribution plots are disabled in config.')\n",
        "else:\n",
        "    print('Skipping training data distribution plots before training.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a5ed59",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Print diagnostic stats for features, conditions, and labels.\"\"\"\n",
        "# Split-level diagnostics: features, conditions, and labels\n",
        "# Diagnostics: input/label/condition stats\n",
        "from localized_entropy.analysis import (\n",
        "    print_condition_stats,\n",
        "    print_feature_stats,\n",
        "    print_label_stats,\n",
        ")\n",
        "\n",
        "print('Diagnostics: splits')\n",
        "# Feature distributions per split\n",
        "print_feature_stats('Train features', splits.x_train)\n",
        "print_feature_stats('Eval features', splits.x_eval)\n",
        "if splits.x_test is not None:\n",
        "    print_feature_stats('Test features', splits.x_test)\n",
        "\n",
        "# Condition counts per split\n",
        "print_condition_stats('Train conds', splits.c_train, splits.num_conditions)\n",
        "print_condition_stats('Eval conds', splits.c_eval, splits.num_conditions)\n",
        "if splits.c_test is not None:\n",
        "    print_condition_stats('Test conds', splits.c_test, splits.num_conditions)\n",
        "\n",
        "# Label base rates per split\n",
        "print_label_stats('Train labels', splits.y_train, splits.c_train, splits.num_conditions)\n",
        "print_label_stats('Eval labels', splits.y_eval, splits.c_eval, splits.num_conditions)\n",
        "if splits.y_test is not None:\n",
        "    print_label_stats('Test labels', splits.y_test, splits.c_test, splits.num_conditions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3d2b1fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Build model instances for each requested loss mode.\"\"\"\n",
        "models = {}\n",
        "for loss_mode in loss_modes:\n",
        "    set_seed(cfg['project']['seed'], use_cuda)\n",
        "    model = build_model(cfg, splits, device, dtype=model_dtype)\n",
        "    models[loss_mode] = model\n",
        "    if len(loss_modes) == 1:\n",
        "        model\n",
        "    else:\n",
        "        print(f\"Model ({loss_label(loss_mode)}):\")\n",
        "        print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46a27d91",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Check initial logits/probabilities before training.\"\"\"\n",
        "# Use a single batch to sanity-check untrained logits/probabilities\n",
        "# Diagnostics: initial logits/prob stats (untrained)\n",
        "try:\n",
        "    batch = next(iter(train_eval_loader))\n",
        "except StopIteration:\n",
        "    batch = None\n",
        "if batch is None:\n",
        "    print(f\"{train_eval_name} loader empty; skipping init logits diagnostics.\")\n",
        "else:\n",
        "    # Batch includes per-sample weights (ignored for init diagnostics)\n",
        "    x_b, x_cat_b, c_b, y_b, _ = batch\n",
        "    x_b = x_b.to(device, non_blocking=non_blocking)\n",
        "    x_cat_b = x_cat_b.to(device, non_blocking=non_blocking)\n",
        "    c_b = c_b.to(device, non_blocking=non_blocking)\n",
        "    for loss_mode, model in models.items():\n",
        "        label = loss_label(loss_mode)\n",
        "        with torch.no_grad():\n",
        "            logits = model(x_b, x_cat_b, c_b)\n",
        "            probs = torch.sigmoid(logits)\n",
        "        logits_np = logits.detach().cpu().numpy().reshape(-1)\n",
        "        probs_np = probs.detach().cpu().numpy().reshape(-1)\n",
        "        print(\n",
        "            f\"{label} init batch logits: n={logits_np.size:,} min={logits_np.min():.6g} \"\n",
        "            f\"max={logits_np.max():.6g} mean={logits_np.mean():.6g} std={logits_np.std():.6g}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"{label} init batch probs:  n={probs_np.size:,} min={probs_np.min():.6g} \"\n",
        "            f\"max={probs_np.max():.6g} mean={probs_np.mean():.6g} std={probs_np.std():.6g}\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "983d54bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Inspect pre-training predictions by condition when enabled.\"\"\"\n",
        "if plots_cfg.get('eval_pred_by_condition', True):\n",
        "    for loss_mode, model in models.items():\n",
        "        loss_name = loss_label(loss_mode)\n",
        "        pretrain_eval_preds = predict_probs(\n",
        "            model,\n",
        "            train_eval_loader,\n",
        "            device,\n",
        "            non_blocking=non_blocking,\n",
        "        )\n",
        "        if pretrain_eval_preds.size > 0:\n",
        "            if train_eval_conds is None:\n",
        "                print(f\"{train_eval_name} conditions unavailable; skipping per-condition stats.\")\n",
        "            else:\n",
        "                print_pred_stats_by_condition(\n",
        "                    pretrain_eval_preds,\n",
        "                    train_eval_conds,\n",
        "                    splits.num_conditions,\n",
        "                    name=f\"Pre-Training {train_eval_name} ({loss_name})\",\n",
        "                )\n",
        "                plot_eval_predictions_by_condition(\n",
        "                    pretrain_eval_preds,\n",
        "                    train_eval_conds,\n",
        "                    splits.num_conditions,\n",
        "                    value_range=eval_value_range,\n",
        "                    title=(\n",
        "                        f\"Pre-Training {train_eval_name} Predictions by {condition_label} (\"\n",
        "                        f\"{loss_name})\"\n",
        "                    ),\n",
        "                )\n",
        "        else:\n",
        "            print(f\"{train_eval_name} set is empty after filtering; skipping pre-training plot.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce01bdc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Train models with optional eval callbacks and collect run results.\"\"\"\n",
        "run_results = {}\n",
        "# Collect logits/targets when LE stats or comparisons need them later.\n",
        "need_eval_logits = (\n",
        "    eval_has_labels\n",
        "    and (\n",
        "        train_bce_le\n",
        "        or plots_cfg.get('le_stats', True)\n",
        "        or plots_cfg.get('print_le_stats_table', True)\n",
        "    )\n",
        ")\n",
        "plot_eval_epochs = plots_cfg.get('eval_pred_by_condition', True)\n",
        "# Resolve per-loss training overrides (batch size, lr, epochs).\n",
        "loss_train_cfgs = {}\n",
        "loss_loaders_by_mode = {}\n",
        "loss_eval_loaders = {}\n",
        "loss_train_eval_loaders = {}\n",
        "for loss_mode in loss_modes:\n",
        "    loss_loaders, loss_train_cfg = build_loss_loaders(cfg, loss_mode, splits, device, use_cuda, use_mps)\n",
        "    loss_train_cfgs[loss_mode] = loss_train_cfg\n",
        "    loss_loaders_by_mode[loss_mode] = loss_loaders\n",
        "    loss_eval_loader = select_eval_loader(eval_split, loss_loaders)\n",
        "    loss_eval_loaders[loss_mode] = loss_eval_loader\n",
        "    loss_train_eval_loader, _, _ = resolve_train_eval_bundle(\n",
        "        eval_split,\n",
        "        loss_eval_loader,\n",
        "        eval_labels,\n",
        "        eval_conds,\n",
        "        eval_name,\n",
        "        loss_loaders,\n",
        "        splits,\n",
        "    )\n",
        "    loss_train_eval_loaders[loss_mode] = loss_train_eval_loader\n",
        "# Run training for each loss mode\n",
        "for loss_mode, model in models.items():\n",
        "    set_seed(cfg['project']['seed'], use_cuda)\n",
        "    loss_name = loss_label(loss_mode)\n",
        "    print(f\"[INFO] Training {loss_name} model\")\n",
        "    loss_train_cfg = loss_train_cfgs[loss_mode]\n",
        "    focal_cfg = loss_train_cfg.get('focal', {}) if isinstance(loss_train_cfg, dict) else {}\n",
        "    focal_alpha = focal_cfg.get('alpha') if isinstance(focal_cfg, dict) else None\n",
        "    focal_gamma = focal_cfg.get('gamma') if isinstance(focal_cfg, dict) else None\n",
        "    eval_every_n_batches = int(loss_train_cfg.get('eval_every_n_batches', 0) or 0)\n",
        "    plot_eval_batches = plot_eval_epochs and eval_every_n_batches > 0\n",
        "    track_eval_batch_losses = plots_cfg.get('loss_curves', True) and eval_every_n_batches > 0\n",
        "    eval_callback = (\n",
        "        build_eval_epoch_plotter(\n",
        "            train_eval_name,\n",
        "            train_eval_conds,\n",
        "            splits.num_conditions,\n",
        "            eval_value_range,\n",
        "            condition_label,\n",
        "            loss_name,\n",
        "        )\n",
        "        if plot_eval_epochs else None\n",
        "    )\n",
        "    eval_batch_callback = (\n",
        "        build_eval_batch_plotter(\n",
        "            train_eval_name,\n",
        "            train_eval_conds,\n",
        "            splits.num_conditions,\n",
        "            eval_value_range,\n",
        "            condition_label,\n",
        "            loss_name,\n",
        "        )\n",
        "        if plot_eval_batches else None\n",
        "    )\n",
        "    lr_category = None\n",
        "    lr_zero_after_epochs = None\n",
        "    if data_source == 'synthetic' and loss_mode == 'localized_entropy':\n",
        "        lr_category = loss_train_cfg.get('lr_category', loss_train_cfg.get('LRCategory'))\n",
        "        lr_zero_after_epochs = loss_train_cfg.get('lr_zero_after_epochs')\n",
        "    le_cross_batch_cfg = None\n",
        "    if loss_mode == 'localized_entropy':\n",
        "        le_cfg = loss_train_cfg.get('localized_entropy')\n",
        "        if isinstance(le_cfg, dict):\n",
        "            le_cross_batch_cfg = le_cfg.get('cross_batch')\n",
        "    run_results[loss_mode] = train_single_loss(\n",
        "        model=model,\n",
        "        loss_mode=loss_mode,\n",
        "        train_loader=loss_loaders_by_mode[loss_mode].train_loader,\n",
        "        train_eval_loader=loss_train_eval_loaders[loss_mode],\n",
        "        eval_loader=loss_eval_loaders[loss_mode],\n",
        "        device=device,\n",
        "        epochs=loss_train_cfg['epochs'],\n",
        "        lr=loss_train_cfg['lr'],\n",
        "        lr_category=lr_category,\n",
        "        lr_zero_after_epochs=lr_zero_after_epochs,\n",
        "        eval_has_labels=eval_has_labels,\n",
        "        le_base_rates_train=le_base_rates_train,\n",
        "        le_base_rates_train_eval=le_base_rates_train_eval,\n",
        "        le_base_rates_eval=le_base_rates_eval,\n",
        "        focal_alpha=focal_alpha,\n",
        "        focal_gamma=focal_gamma,\n",
        "        non_blocking=non_blocking,\n",
        "        plot_eval_hist_epochs=plots_cfg.get('eval_hist_epochs', False),\n",
        "        eval_callback=eval_callback,\n",
        "        eval_every_n_batches=(\n",
        "            eval_every_n_batches if (plot_eval_batches or track_eval_batch_losses) else None\n",
        "        ),\n",
        "        eval_batch_callback=eval_batch_callback,\n",
        "        collect_eval_batch_losses=track_eval_batch_losses,\n",
        "        collect_grad_sq_sums=plots_cfg.get('grad_sq_by_condition', False),\n",
        "        debug_gradients=loss_train_cfg.get('debug_gradients', False),\n",
        "        le_cross_batch_cfg=le_cross_batch_cfg,\n",
        "        print_embedding_table=loss_train_cfg.get('print_embedding_table', False),\n",
        "        collect_eval_logits=need_eval_logits,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db381ad8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Plot training/eval loss curves.\"\"\"\n",
        "if plots_cfg.get('loss_curves', True):\n",
        "    for loss_mode, result in run_results.items():\n",
        "        output_path = output_paths[loss_mode][\"loss_curves\"]\n",
        "        plot_loss_curves(\n",
        "            result.train_losses,\n",
        "            result.eval_losses,\n",
        "            result.loss_label,\n",
        "            output_path=output_path,\n",
        "            eval_batch_losses=result.eval_batch_losses if eval_every_n_batches > 0 else None,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee164192",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Evaluate trained models, compute metrics, and render plots.\"\"\"\n",
        "eval_cfg = cfg.get('evaluation', {})\n",
        "for loss_mode, result in run_results.items():\n",
        "    label = result.loss_label\n",
        "    eval_preds = result.eval_preds\n",
        "    eval_loss = result.eval_loss\n",
        "    model = result.model\n",
        "    loss_train_cfg = loss_train_cfgs.get(loss_mode, train_cfg)\n",
        "    epochs_completed = max(0, len(result.train_losses) - 1)\n",
        "    display_name = eval_name if not train_multi else f\"{eval_name} ({label})\"\n",
        "    print(f\"=== {label} model evaluation ===\")\n",
        "    # Summary + plots\n",
        "    if plots_cfg.get('print_eval_summary', True):\n",
        "        print_pred_summary(display_name, eval_preds, labels=eval_labels, conds=eval_conds)\n",
        "    if plots_cfg.get('eval_pred_hist', True):\n",
        "        plot_eval_log10p_hist(eval_preds.astype(np.float32), epoch=epochs_completed, name=display_name)\n",
        "    if plots_cfg.get('eval_pred_by_condition', True):\n",
        "        if eval_conds is None:\n",
        "            print(f\"{display_name} conditions unavailable; skipping eval predictions by condition.\")\n",
        "        else:\n",
        "            plot_eval_predictions_by_condition(\n",
        "                eval_preds,\n",
        "                eval_conds,\n",
        "                splits.num_conditions,\n",
        "                value_range=eval_value_range,\n",
        "                name=display_name,\n",
        "                output_path=output_paths[loss_mode][\"post_training_eval_predictions\"],\n",
        "                title=(\n",
        "                    f\"Post-Training {display_name} Predictions by {condition_label} (\"\n",
        "                    f\"{label})\"\n",
        "                ),\n",
        "            )\n",
        "            print(\"DEBUG: PREDICTIONS ARRAY\")\n",
        "            print(eval_preds[:100])\n",
        "    # Metrics that require labels\n",
        "    if eval_has_labels:\n",
        "        print(f\"Final {eval_name} {label}: {eval_loss:.10f}\")\n",
        "    else:\n",
        "        print(f\"Final {eval_name} {label}: n/a (labels unavailable)\")\n",
        "    if eval_has_labels:\n",
        "        bins = int(eval_cfg.get('ece_bins', 20))\n",
        "        min_count = int(eval_cfg.get('ece_min_count', 1))\n",
        "        small_prob_max_cfg = float(eval_cfg.get('small_prob_max', 0.01))\n",
        "        small_prob_quantile = float(eval_cfg.get('small_prob_quantile', 0.1))\n",
        "        total_bce = bce_log_loss(eval_preds, eval_labels)\n",
        "        total_ece, total_ece_table = expected_calibration_error(\n",
        "            eval_preds, eval_labels, bins=bins, min_count=min_count\n",
        "        )\n",
        "        # Define low-probability calibration subset; fall back to a quantile if needed.\n",
        "        small_threshold = small_prob_max_cfg\n",
        "        small_mask = eval_preds <= small_threshold\n",
        "        if not small_mask.any():\n",
        "            quantile_threshold = float(np.quantile(eval_preds, small_prob_quantile))\n",
        "            print(\n",
        "                f\"[INFO] No preds <= {small_threshold:g}; using {small_prob_quantile:.2f} quantile \"\n",
        "                f\"threshold {quantile_threshold:g} for small-prob calibration.\"\n",
        "            )\n",
        "            small_threshold = quantile_threshold\n",
        "            small_mask = eval_preds <= small_threshold\n",
        "        if small_mask.any():\n",
        "            total_ece_small, _ = expected_calibration_error(\n",
        "                eval_preds[small_mask],\n",
        "                eval_labels[small_mask],\n",
        "                bins=bins,\n",
        "                min_count=min_count,\n",
        "            )\n",
        "        else:\n",
        "            total_ece_small = float('nan')\n",
        "        print(f\"{label} Total BCE (log loss): {total_bce:.8f}\")\n",
        "        print(f\"{label} Total ECE: {total_ece:.8f}\")\n",
        "        print(f\"{label} Total ECE (small p<= {small_threshold:g}): {total_ece_small:.8f}\")\n",
        "        total_auc = roc_auc_score(eval_preds, eval_labels)\n",
        "        total_pr_auc = pr_auc_score(eval_preds, eval_labels)\n",
        "        print(f\"{label} Total ROC-AUC: {total_auc:.8f}\")\n",
        "        print(f\"{label} Total PR-AUC (AP): {total_pr_auc:.8f}\")\n",
        "        cls_metrics = binary_classification_metrics(eval_preds, eval_labels)\n",
        "        print(f\"{label} Total Accuracy@0.5: {cls_metrics['accuracy']:.8f}\")\n",
        "        print(f\"{label} Total F1@0.5: {cls_metrics['f1']:.8f}\")\n",
        "        if eval_cfg.get('print_calibration_table', False):\n",
        "            print(total_ece_table.to_string(index=False))\n",
        "        if eval_conds is None:\n",
        "            print(f\"[WARN] {display_name} conditions unavailable; skipping per-condition metrics.\")\n",
        "        else:\n",
        "            per_ad = per_condition_metrics(\n",
        "                eval_preds,\n",
        "                eval_labels,\n",
        "                eval_conds,\n",
        "                bins=bins,\n",
        "                min_count=min_count,\n",
        "                small_prob_max=small_threshold,\n",
        "            )\n",
        "            if eval_cfg.get('print_per_ad', True):\n",
        "                top_k = int(eval_cfg.get('per_ad_top_k', 10))\n",
        "                print(per_ad.head(top_k).to_string(index=False))\n",
        "        if plots_cfg.get('eval_calibration_ratio', True):\n",
        "            if eval_conds is None:\n",
        "                print(f\"[WARN] {display_name} conditions unavailable; skipping calibration ratio plot.\")\n",
        "            else:\n",
        "                cal_df = None\n",
        "                if use_true_le_base_rates and le_base_rates_eval is not None:\n",
        "                    cal_df = per_condition_calibration_from_base_rates(\n",
        "                        eval_preds,\n",
        "                        eval_conds,\n",
        "                        le_base_rates_eval,\n",
        "                    )\n",
        "                elif eval_has_labels:\n",
        "                    cal_df = per_condition_calibration(\n",
        "                        eval_preds,\n",
        "                        eval_labels,\n",
        "                        eval_conds,\n",
        "                    )\n",
        "                if cal_df is None:\n",
        "                    print(f\"[WARN] {display_name} calibration data unavailable; skipping calibration ratio plot.\")\n",
        "                else:\n",
        "                    plot_calibration_ratio_by_condition(\n",
        "                        cal_df['base_rate'].to_numpy(),\n",
        "                        cal_df['calibration'].to_numpy(),\n",
        "                        name=display_name,\n",
        "                        condition_label=condition_label,\n",
        "                        title=f\"{display_name} Calibration Ratio vs {condition_label} Base Rate\",\n",
        "                        output_path=output_paths[loss_mode]['calibration_ratio'],\n",
        "                    )\n",
        "        if not train_multi:\n",
        "            focal_cfg = loss_train_cfg.get('focal', {}) if isinstance(loss_train_cfg, dict) else {}\n",
        "            focal_alpha = focal_cfg.get('alpha') if isinstance(focal_cfg, dict) else None\n",
        "            focal_gamma = focal_cfg.get('gamma') if isinstance(focal_cfg, dict) else None\n",
        "            for mode in loss_train_cfg.get('eval_compare_losses', []):\n",
        "                if mode == loss_mode:\n",
        "                    continue\n",
        "                other_base_rates = (\n",
        "                    le_base_rates_eval if (use_true_le_base_rates and mode == 'localized_entropy') else None\n",
        "                )\n",
        "                other_loss, _ = evaluate(\n",
        "                    model,\n",
        "                    eval_loader,\n",
        "                    device,\n",
        "                    loss_mode=mode,\n",
        "                    focal_alpha=focal_alpha,\n",
        "                    focal_gamma=focal_gamma,\n",
        "                    base_rates=other_base_rates,\n",
        "                    non_blocking=non_blocking,\n",
        "                )\n",
        "                other_label = loss_label(mode)\n",
        "                print(f\"Final {eval_name} {other_label}: {other_loss:.10f}\")\n",
        "    # Per-ad train click rates vs. eval prediction averages\n",
        "    plot_df = summarize_per_ad_train_eval_rates(\n",
        "        splits.y_train,\n",
        "        splits.c_train,\n",
        "        eval_preds,\n",
        "        eval_conds,\n",
        "        splits.num_conditions,\n",
        "        condition_label=condition_label,\n",
        "        eval_name=eval_name,\n",
        "        top_k=eval_cfg.get('per_ad_top_k', 10),\n",
        "    )\n",
        "    if plot_df is not None:\n",
        "        plot_pred_to_train_rate(\n",
        "            plot_df,\n",
        "            condition_label=condition_label,\n",
        "            eval_name=eval_name,\n",
        "            output_path=output_paths[loss_mode][\"pred_to_train_rate\"],\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "53859dfa",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Run optional test-set inference/plots when configured.\"\"\"\n",
        "if loaders.test_loader is not None and eval_split != 'test':\n",
        "    for loss_mode, result in run_results.items():\n",
        "        label = result.loss_label\n",
        "        display_name = 'Test' if not train_multi else f\"Test ({label})\"\n",
        "        test_preds = predict_probs(\n",
        "            result.model,\n",
        "            loaders.test_loader,\n",
        "            device,\n",
        "            non_blocking=non_blocking,\n",
        "        )\n",
        "        if test_preds.size > 0:\n",
        "            print_pred_summary(display_name, test_preds, labels=None, conds=splits.c_test)\n",
        "            if plots_cfg.get('eval_pred_hist', True):\n",
        "                plot_eval_log10p_hist(\n",
        "                    test_preds.astype(np.float32),\n",
        "                    epoch=train_cfg['epochs'],\n",
        "                    name=display_name,\n",
        "                )\n",
        "            if plots_cfg.get('eval_pred_by_condition', True):\n",
        "                if splits.c_test is None:\n",
        "                    print(f\"{display_name} conditions unavailable; skipping test predictions by condition.\")\n",
        "                else:\n",
        "                    plot_eval_predictions_by_condition(\n",
        "                        test_preds,\n",
        "                        splits.c_test,\n",
        "                        splits.num_conditions,\n",
        "                        value_range=eval_value_range,\n",
        "                        name=display_name,\n",
        "                        title=(\n",
        "                            f\"Post-Training {display_name} Predictions by {condition_label} (\"\n",
        "                            f\"{label})\"\n",
        "                        ),\n",
        "                    )\n",
        "        else:\n",
        "            print(f\"{display_name} set is empty after filtering; skipping test plot.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e419b23",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Compute localized entropy stats per condition and plot tables.\"\"\"\n",
        "need_le_stats = (\n",
        "    plots_cfg.get('le_stats', True)\n",
        "    or plots_cfg.get('print_le_stats_table', True)\n",
        "    or train_bce_le\n",
        ")\n",
        "if need_le_stats:\n",
        "    if not eval_has_labels:\n",
        "        print(f\"{eval_name} labels unavailable; skipping localized entropy stats.\")\n",
        "    elif eval_conds is None:\n",
        "        print(f\"{eval_name} conditions unavailable; skipping localized entropy stats.\")\n",
        "    else:\n",
        "        for loss_mode, result in run_results.items():\n",
        "            label = result.loss_label\n",
        "            z_all = result.eval_logits\n",
        "            y_all = result.eval_targets\n",
        "            c_all = result.eval_conds\n",
        "            if z_all is None or y_all is None or c_all is None:\n",
        "                z_all, y_all, c_all = collect_logits(\n",
        "                    result.model, eval_loader, device, non_blocking=non_blocking\n",
        "                )\n",
        "                result.eval_logits = z_all\n",
        "                result.eval_targets = y_all\n",
        "                result.eval_conds = c_all\n",
        "            le_stats = collect_le_stats_per_condition(z_all, y_all, c_all, eps=1e-12)\n",
        "            if plots_cfg.get('print_le_stats_table', True):\n",
        "                print(f\"LE stats table ({label})\")\n",
        "                print('cond\tnum\tden\tavg_p\t#y=1\t#y=0\tratio')\n",
        "                for cond in sorted(le_stats.keys()):\n",
        "                    s = le_stats[cond]\n",
        "                    print(\n",
        "                        f\"{cond}\t{s['Numerator']:.6g}\t{s['Denominator']:.6g}\t\"\n",
        "                        f\"{s['Average prediction for denominator']:.6g}\t\"\n",
        "                        f\"{s['Number of samples with label 1']}\t\"\n",
        "                        f\"{s['Number of samples with label 0']}\t\"\n",
        "                        f\"{s['Numerator/denominator']:.6g}\"\n",
        "                    )\n",
        "            if plots_cfg.get('le_stats', True):\n",
        "                plot_le_stats_per_condition(\n",
        "                    le_stats,\n",
        "                    title=f\"Localized Entropy terms per condition - {eval_name} set ({label})\",\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a91fde3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Compare BCE vs LE and plot gradient diagnostics if enabled.\"\"\"\n",
        "compare_cfg = cfg.get('comparison', {})\n",
        "compare_enabled = compare_cfg.get('enabled', True)\n",
        "has_bce = 'bce' in run_results\n",
        "has_le = 'localized_entropy' in run_results\n",
        "has_focal = 'focal' in run_results\n",
        "if train_bce_le and compare_enabled:\n",
        "    if not eval_has_labels or eval_conds is None:\n",
        "        print(f\"[WARN] {eval_name} labels/conditions unavailable; skipping BCE vs LE comparison.\")\n",
        "    else:\n",
        "        bce_result = run_results.get('bce')\n",
        "        le_result = run_results.get('localized_entropy')\n",
        "        if bce_result is None or le_result is None:\n",
        "            print(\"[WARN] Missing BCE or LE results; skipping comparison.\")\n",
        "        else:\n",
        "            comparison = compare_bce_le_runs(\n",
        "                bce_result,\n",
        "                le_result,\n",
        "                eval_labels,\n",
        "                eval_conds,\n",
        "                condition_label=condition_label,\n",
        "                sort_by=str(compare_cfg.get('sort_by', 'count')),\n",
        "            )\n",
        "            comparison_table = comparison.sort_values(condition_label)\n",
        "            if compare_cfg.get('print_table', True):\n",
        "                table_columns = [\n",
        "                    condition_label,\n",
        "                    'count',\n",
        "                    'base_rate',\n",
        "                    'bce_pred_mean',\n",
        "                    'le_pred_mean',\n",
        "                    'bce_logloss',\n",
        "                    'le_logloss',\n",
        "                    'bce_calibration',\n",
        "                    'le_calibration',\n",
        "                    'delta_calibration',\n",
        "                    'bce_le_ratio',\n",
        "                    'le_le_ratio',\n",
        "                    'delta_le_ratio',\n",
        "                ]\n",
        "                title = f\"{eval_name}: per-{condition_label} BCE vs LE comparison\"\n",
        "                print(title)\n",
        "                print(format_comparison_table(\n",
        "                    comparison_table,\n",
        "                    table_columns,\n",
        "                    top_k=int(compare_cfg.get('top_k', 20)),\n",
        "                ))\n",
        "                print('')\n",
        "                cal_table = comparison_table[[condition_label, 'bce_calibration', 'le_calibration']].copy()\n",
        "                if has_focal:\n",
        "                    focal_result = run_results.get('focal')\n",
        "                    if focal_result is not None:\n",
        "                        focal_cal = per_condition_calibration(\n",
        "                            focal_result.eval_preds,\n",
        "                            eval_labels,\n",
        "                            eval_conds,\n",
        "                        )\n",
        "                        focal_cal = focal_cal.rename(\n",
        "                            columns={'calibration': 'fl_calibration'}\n",
        "                        )\n",
        "                        cal_table = cal_table.merge(\n",
        "                            focal_cal[[\"condition\", 'fl_calibration']],\n",
        "                            left_on=condition_label,\n",
        "                            right_on='condition',\n",
        "                            how='left',\n",
        "                        ).drop(columns=['condition'])\n",
        "                cal_table['bce_abs_1_minus_cal'] = (1.0 - cal_table['bce_calibration']).abs()\n",
        "                cal_table['le_abs_1_minus_cal'] = (1.0 - cal_table['le_calibration']).abs()\n",
        "                if 'fl_calibration' in cal_table.columns:\n",
        "                    cal_table['fl_abs_1_minus_cal'] = (1.0 - cal_table['fl_calibration']).abs()\n",
        "                cal_title = (\n",
        "                    f\"{eval_name}: per-{condition_label} abs(1 - calibration) (lower is better)\"\n",
        "                )\n",
        "                print(cal_title)\n",
        "                print(format_comparison_table(\n",
        "                    cal_table.sort_values(condition_label),\n",
        "                    [\n",
        "                        condition_label,\n",
        "                        'bce_abs_1_minus_cal',\n",
        "                        'le_abs_1_minus_cal',\n",
        "                        *(['fl_abs_1_minus_cal'] if 'fl_abs_1_minus_cal' in cal_table.columns else []),\n",
        "                    ],\n",
        "                    top_k=int(compare_cfg.get('top_k', 20)),\n",
        "                ))\n",
        "                print('')\n",
        "                eval_cfg = cfg.get('evaluation', {})\n",
        "                print(format_bce_le_summary(\n",
        "                    bce_result,\n",
        "                    le_result,\n",
        "                    eval_labels,\n",
        "                    eval_conds=eval_conds,\n",
        "                    condition_label=condition_label,\n",
        "                    ece_bins=int(eval_cfg.get('ece_bins', 20)),\n",
        "                    ece_min_count=int(eval_cfg.get('ece_min_count', 1)),\n",
        "                    small_prob_max=float(eval_cfg.get('small_prob_max', 0.01)),\n",
        "                    small_prob_quantile=float(eval_cfg.get('small_prob_quantile', 0.1)),\n",
        "                ))\n",
        "if compare_enabled and has_focal:\n",
        "    if not eval_has_labels:\n",
        "        print(f\"[WARN] {eval_name} labels unavailable; skipping focal comparison metrics.\")\n",
        "    else:\n",
        "        eval_cfg = cfg.get('evaluation', {})\n",
        "        loss_order = [\n",
        "            ('bce', 'BCE'),\n",
        "            ('localized_entropy', 'LE'),\n",
        "            ('focal', 'Focal'),\n",
        "        ]\n",
        "        metrics_rows = []\n",
        "        for key, name in loss_order:\n",
        "            result = run_results.get(key)\n",
        "            if result is None:\n",
        "                continue\n",
        "            metrics = summarize_model_metrics(\n",
        "                result.eval_preds,\n",
        "                eval_labels,\n",
        "                ece_bins=int(eval_cfg.get('ece_bins', 20)),\n",
        "                ece_min_count=int(eval_cfg.get('ece_min_count', 1)),\n",
        "                threshold=0.5,\n",
        "                small_prob_max=float(eval_cfg.get('small_prob_max', 0.01)),\n",
        "                small_prob_quantile=float(eval_cfg.get('small_prob_quantile', 0.1)),\n",
        "            )\n",
        "            metrics_rows.append((name, metrics))\n",
        "        if metrics_rows:\n",
        "            columns = ['metric'] + [name for name, _ in metrics_rows]\n",
        "            metrics_frame = pd.DataFrame(\n",
        "                {\n",
        "                    'metric': ['accuracy@0.5', 'logloss', 'brier', 'ece', 'ece_small'],\n",
        "                    **{\n",
        "                        name: [\n",
        "                            metrics['accuracy'],\n",
        "                            metrics['logloss'],\n",
        "                            metrics['brier'],\n",
        "                            metrics['ece'],\n",
        "                            metrics['ece_small'],\n",
        "                        ]\n",
        "                        for name, metrics in metrics_rows\n",
        "                    },\n",
        "                }\n",
        "            )\n",
        "            title = \"BCE vs LE vs Focal summary (lower is better for logloss/brier/ece):\"\n",
        "            print(title)\n",
        "            print(format_comparison_table(metrics_frame, columns, top_k=len(metrics_frame)))\n",
        "\n",
        "grad_plot_enabled = plots_cfg.get('grad_sq_by_condition', False)\n",
        "if grad_plot_enabled:\n",
        "    if not train_bce_le:\n",
        "        print('[WARN] Grad plot requires BCE and LE runs; skipping.')\n",
        "    else:\n",
        "        bce_result = run_results.get('bce')\n",
        "        le_result = run_results.get('localized_entropy')\n",
        "        if bce_result is None or le_result is None:\n",
        "            print('[WARN] Missing BCE or LE results; skipping grad plot.')\n",
        "        else:\n",
        "            bce_grad_stats = bce_result.grad_sq_stats\n",
        "            le_grad_stats = le_result.grad_sq_stats\n",
        "            if bce_grad_stats is None or le_grad_stats is None:\n",
        "                print('[WARN] Grad stats not collected; enable plots.grad_sq_by_condition to track.')\n",
        "            else:\n",
        "                bce_grads = bce_grad_stats.mean_by_condition\n",
        "                le_grads = le_grad_stats.mean_by_condition\n",
        "                plot_grad_sq_sums_by_condition(\n",
        "                    bce_grads,\n",
        "                    le_grads,\n",
        "                    condition_label=condition_label,\n",
        "                    title=(\n",
        "                        f\"Training gradient mean square by {condition_label} (BCE vs LE)\"\n",
        "                    ),\n",
        "                    top_k=int(plots_cfg.get('grad_sq_top_k', 0) or 0),\n",
        "                    log10=bool(plots_cfg.get('grad_sq_log10', True)),\n",
        "                )\n",
        "\n",
        "                grad_cond_ids = np.arange(len(bce_grad_stats.mean_by_condition))\n",
        "                grad_top_k = int(plots_cfg.get('grad_sq_top_k', 0) or 0)\n",
        "                if grad_top_k > 0 and grad_top_k < grad_cond_ids.size:\n",
        "                    scores = np.maximum(\n",
        "                        bce_grad_stats.mean_by_condition,\n",
        "                        le_grad_stats.mean_by_condition,\n",
        "                    )\n",
        "                    grad_cond_ids = np.argsort(scores)[::-1][:grad_top_k]\n",
        "                bce_vals = bce_grad_stats.mean_by_condition[grad_cond_ids]\n",
        "                le_vals = le_grad_stats.mean_by_condition[grad_cond_ids]\n",
        "                ratio = np.divide(\n",
        "                    le_vals,\n",
        "                    bce_vals,\n",
        "                    out=np.full_like(le_vals, np.nan),\n",
        "                    where=bce_vals != 0,\n",
        "                )\n",
        "                grad_table = pd.DataFrame(\n",
        "                    {\n",
        "                        condition_label: grad_cond_ids,\n",
        "                        'bce_grad_mse': bce_vals,\n",
        "                        'le_grad_mse': le_vals,\n",
        "                        'le_over_bce_grad_mse': ratio,\n",
        "                    }\n",
        "                )\n",
        "                grad_title = (\n",
        "                    f\"Training per-{condition_label} grad MSE ratio (LE/BCE)\"\n",
        "                )\n",
        "                print(grad_title)\n",
        "                print(format_comparison_table(\n",
        "                    grad_table,\n",
        "                    [\n",
        "                        condition_label,\n",
        "                        'bce_grad_mse',\n",
        "                        'le_grad_mse',\n",
        "                        'le_over_bce_grad_mse',\n",
        "                    ],\n",
        "                    top_k=len(grad_table),\n",
        "                ))\n",
        "                print('')\n",
        "\n",
        "                def _print_grad_class_mse(label, stats):\n",
        "                    mse = stats.class_mse\n",
        "                    counts = stats.class_counts\n",
        "                    ratio = stats.class_ratio\n",
        "                    if mse is None or counts is None:\n",
        "                        print(f\"[WARN] {label} grad class MSE unavailable.\")\n",
        "                        return\n",
        "                    count0 = int(counts[0]) if np.isfinite(counts[0]) else 0\n",
        "                    count1 = int(counts[1]) if np.isfinite(counts[1]) else 0\n",
        "                    ratio_str = f\"{ratio:.4f}\" if np.isfinite(ratio) else \"nan\"\n",
        "                    print(\n",
        "                        f\"{label} grad MSE by class: \"\n",
        "                        f\"0={mse[0]:.6e} (n={count0}), \"\n",
        "                        f\"1={mse[1]:.6e} (n={count1}), \"\n",
        "                        f\"ratio={ratio_str}\"\n",
        "                    )\n",
        "\n",
        "                _print_grad_class_mse('BCE', bce_grad_stats)\n",
        "                _print_grad_class_mse('LE', le_grad_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7a426193",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Run repeated experiments and statistical comparisons.\"\"\"\n",
        "repeat_cfg = cfg.get('repeats', {})\n",
        "repeat_enabled = bool(repeat_cfg.get('enabled', False))\n",
        "repeat_runs = int(repeat_cfg.get('num_runs', 1) or 0)\n",
        "repeat_seed_stride = int(repeat_cfg.get('seed_stride', 1) or 1)\n",
        "include_base_run = bool(repeat_cfg.get('include_base_run', True))\n",
        "if repeat_enabled:\n",
        "    if repeat_runs < 1:\n",
        "        print('[WARN] repeats.enabled is true but repeats.num_runs < 1; skipping repeats.')\n",
        "    elif not train_bce_le:\n",
        "        print('[WARN] Repeat runs require training BCE + LE; skipping.')\n",
        "    elif not eval_has_labels:\n",
        "        print(f\"[WARN] {eval_name} labels unavailable; skipping repeat stats.\")\n",
        "    else:\n",
        "        base_seed = int(cfg['project']['seed'])\n",
        "        seeds = build_seed_sequence(base_seed, repeat_runs, repeat_seed_stride)\n",
        "        repeat_results = {loss_mode: [] for loss_mode in loss_modes}\n",
        "        run_seeds = []\n",
        "        start_idx = 0\n",
        "        # Reuse the already-computed base run when possible to avoid retraining.\n",
        "        if include_base_run:\n",
        "            missing = [mode for mode in loss_modes if mode not in run_results]\n",
        "            if missing:\n",
        "                print(f\"[WARN] Missing base run results for {missing}; running all repeats from scratch.\")\n",
        "            else:\n",
        "                for loss_mode in loss_modes:\n",
        "                    repeat_results[loss_mode].append(run_results[loss_mode])\n",
        "                run_seeds.append(seeds[0] if seeds else base_seed)\n",
        "                start_idx = 1\n",
        "        extra_seeds = seeds[start_idx:]\n",
        "        # Train additional seeds for significance testing.\n",
        "        if extra_seeds:\n",
        "            print(f\"[INFO] Running {len(extra_seeds)} repeated runs for significance testing.\")\n",
        "            extra_results = run_repeated_loss_experiments(\n",
        "                cfg=cfg,\n",
        "                loss_modes=loss_modes,\n",
        "                splits=splits,\n",
        "                eval_split=eval_split,\n",
        "                eval_labels=eval_labels,\n",
        "                eval_conds=eval_conds,\n",
        "                eval_name=eval_name,\n",
        "                device=device,\n",
        "                use_cuda=use_cuda,\n",
        "                use_mps=use_mps,\n",
        "                model_dtype=model_dtype,\n",
        "                seeds=extra_seeds,\n",
        "                le_base_rates_train=le_base_rates_train,\n",
        "                le_base_rates_train_eval=le_base_rates_train_eval,\n",
        "                le_base_rates_eval=le_base_rates_eval,\n",
        "                non_blocking=non_blocking,\n",
        "                collect_eval_logits=False,\n",
        "            )\n",
        "            for loss_mode, runs in extra_results.items():\n",
        "                repeat_results[loss_mode].extend(runs)\n",
        "            run_seeds.extend(extra_seeds)\n",
        "        bce_runs = repeat_results.get('bce', [])\n",
        "        le_runs = repeat_results.get('localized_entropy', [])\n",
        "        focal_runs = repeat_results.get('focal', [])\n",
        "        if len(bce_runs) != len(le_runs):\n",
        "            print('[WARN] Repeat run counts do not match between BCE and LE; skipping Wilcoxon test.')\n",
        "        elif not bce_runs:\n",
        "            print('[WARN] No repeated runs available for significance testing.')\n",
        "        else:\n",
        "            eval_cfg = cfg.get('evaluation', {})\n",
        "            run_values = run_seeds if len(run_seeds) == len(bce_runs) else None\n",
        "            bce_metrics = build_repeat_metrics_frame(\n",
        "                bce_runs,\n",
        "                eval_labels,\n",
        "                ece_bins=int(eval_cfg.get('ece_bins', 20)),\n",
        "                ece_min_count=int(eval_cfg.get('ece_min_count', 1)),\n",
        "                threshold=0.5,\n",
        "                small_prob_max=float(eval_cfg.get('small_prob_max', 0.01)),\n",
        "                small_prob_quantile=float(eval_cfg.get('small_prob_quantile', 0.1)),\n",
        "                run_label='seed',\n",
        "                run_values=run_values,\n",
        "            )\n",
        "            le_metrics = build_repeat_metrics_frame(\n",
        "                le_runs,\n",
        "                eval_labels,\n",
        "                ece_bins=int(eval_cfg.get('ece_bins', 20)),\n",
        "                ece_min_count=int(eval_cfg.get('ece_min_count', 1)),\n",
        "                threshold=0.5,\n",
        "                small_prob_max=float(eval_cfg.get('small_prob_max', 0.01)),\n",
        "                small_prob_quantile=float(eval_cfg.get('small_prob_quantile', 0.1)),\n",
        "                run_label='seed',\n",
        "                run_values=run_values,\n",
        "            )\n",
        "            print('Repeated-run metric summary (BCE):')\n",
        "            print(format_comparison_table(\n",
        "                summarize_repeat_metrics(bce_metrics),\n",
        "                ['metric', 'n', 'mean', 'std', 'min', 'max'],\n",
        "                top_k=10,\n",
        "            ))\n",
        "            print('')\n",
        "            print('Repeated-run metric summary (LE):')\n",
        "            print(format_comparison_table(\n",
        "                summarize_repeat_metrics(le_metrics),\n",
        "                ['metric', 'n', 'mean', 'std', 'min', 'max'],\n",
        "                top_k=10,\n",
        "            ))\n",
        "            print('')\n",
        "            if len(bce_runs) < 2:\n",
        "                print('[WARN] Need at least 2 repeats for a Wilcoxon test; skipping.')\n",
        "            else:\n",
        "                wilcoxon_summary = build_wilcoxon_summary(\n",
        "                    bce_metrics,\n",
        "                    le_metrics,\n",
        "                    zero_method=str(repeat_cfg.get('wilcoxon_zero_method', 'wilcox')),\n",
        "                    alternative=str(repeat_cfg.get('wilcoxon_alternative', 'two-sided')),\n",
        "                )\n",
        "                print('Wilcoxon signed-rank test (delta > 0 favors LE):')\n",
        "                print(format_wilcoxon_summary(wilcoxon_summary))\n",
        "                if eval_conds is None:\n",
        "                    print('[WARN] Eval conditions unavailable; skipping per-condition calibration test.')\n",
        "                else:\n",
        "                    per_cond_min_count = int(repeat_cfg.get('per_condition_min_count', 1))\n",
        "                    per_cond_sort_by = str(repeat_cfg.get('per_condition_sort_by', 'p_value'))\n",
        "                    per_cond_top_k = int(repeat_cfg.get('per_condition_top_k', 20))\n",
        "                    per_cond_summary = build_per_condition_calibration_wilcoxon(\n",
        "                        bce_runs,\n",
        "                        le_runs,\n",
        "                        eval_labels,\n",
        "                        eval_conds,\n",
        "                        zero_method=str(repeat_cfg.get('wilcoxon_zero_method', 'wilcox')),\n",
        "                        alternative=str(repeat_cfg.get('wilcoxon_alternative', 'two-sided')),\n",
        "                        min_count=per_cond_min_count,\n",
        "                    )\n",
        "                    if per_cond_summary is None or len(per_cond_summary) == 0:\n",
        "                        print('[WARN] Per-condition calibration test returned no rows.')\n",
        "                    else:\n",
        "                        per_cond_summary = per_cond_summary.rename(columns={'condition': condition_label})\n",
        "                        per_cond_summary = sort_per_condition_wilcoxon_frame(per_cond_summary, per_cond_sort_by)\n",
        "                        print(f\"Per-{condition_label} calibration Wilcoxon (abs pred_mean - base_rate):\")\n",
        "                        print(format_comparison_table(\n",
        "                            per_cond_summary,\n",
        "                            [condition_label, 'count', 'base_rate', 'bce_gap', 'le_gap', 'delta_mean', 'p_value'],\n",
        "                            top_k=per_cond_top_k,\n",
        "                        ))\n",
        "            if focal_runs:\n",
        "                focal_metrics = build_repeat_metrics_frame(\n",
        "                    focal_runs,\n",
        "                    eval_labels,\n",
        "                    ece_bins=int(eval_cfg.get('ece_bins', 20)),\n",
        "                    ece_min_count=int(eval_cfg.get('ece_min_count', 1)),\n",
        "                    threshold=0.5,\n",
        "                    small_prob_max=float(eval_cfg.get('small_prob_max', 0.01)),\n",
        "                    small_prob_quantile=float(eval_cfg.get('small_prob_quantile', 0.1)),\n",
        "                    run_label='seed',\n",
        "                    run_values=run_values,\n",
        "                )\n",
        "                print('')\n",
        "                print('Repeated-run metric summary (Focal):')\n",
        "                print(format_comparison_table(\n",
        "                    summarize_repeat_metrics(focal_metrics),\n",
        "                    ['metric', 'n', 'mean', 'std', 'min', 'max'],\n",
        "                    top_k=10,\n",
        "                ))\n",
        "                print('')\n",
        "                if len(focal_runs) != len(bce_runs):\n",
        "                    print('[WARN] Repeat run counts do not match between BCE and Focal; skipping Wilcoxon test.')\n",
        "                elif len(focal_runs) < 2:\n",
        "                    print('[WARN] Need at least 2 repeats for a Wilcoxon test (BCE vs Focal); skipping.')\n",
        "                else:\n",
        "                    focal_vs_bce = build_wilcoxon_summary(\n",
        "                        bce_metrics,\n",
        "                        focal_metrics,\n",
        "                        zero_method=str(repeat_cfg.get('wilcoxon_zero_method', 'wilcox')),\n",
        "                        alternative=str(repeat_cfg.get('wilcoxon_alternative', 'two-sided')),\n",
        "                    )\n",
        "                    print('Wilcoxon signed-rank test (delta > 0 favors Focal) [BCE vs Focal]:')\n",
        "                    print(format_wilcoxon_summary(focal_vs_bce))\n",
        "                if len(focal_runs) != len(le_runs):\n",
        "                    print('[WARN] Repeat run counts do not match between LE and Focal; skipping Wilcoxon test.')\n",
        "                elif len(focal_runs) < 2:\n",
        "                    print('[WARN] Need at least 2 repeats for a Wilcoxon test (LE vs Focal); skipping.')\n",
        "                else:\n",
        "                    focal_vs_le = build_wilcoxon_summary(\n",
        "                        le_metrics,\n",
        "                        focal_metrics,\n",
        "                        zero_method=str(repeat_cfg.get('wilcoxon_zero_method', 'wilcox')),\n",
        "                        alternative=str(repeat_cfg.get('wilcoxon_alternative', 'two-sided')),\n",
        "                    )\n",
        "                    print('Wilcoxon signed-rank test (delta > 0 favors Focal) [LE vs Focal]:')\n",
        "                    print(format_wilcoxon_summary(focal_vs_le))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0331edd5",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Stop notebook output capture.\"\"\"\n",
        "if \"notebook_output_capture\" in globals():\n",
        "    if hasattr(notebook_output_capture, \"stop\"):\n",
        "        notebook_output_capture.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
