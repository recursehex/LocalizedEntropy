{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localized Entropy Coss Function\n",
    "\n",
    "- Inputs: Age (float), NetWorth (float), MedicalCondition (categorical via embedding).\n",
    "- Probabilities follow the generated data and are converted to labels with numpy binomial (n=1).\n",
    "- Training uses only labels (not true probabilities).\n",
    "- Pre-training: plot N-line distributions on training data for: log10(NetWorth), Age, and log10(true probability).\n",
    "- Split 90/10 into train/eval.\n",
    "- Model includes dropout; trains with BCE (BCEWithLogitsLoss), batch size=10,000.\n",
    "- Configurable epochs (default 30) and learning rate.\n",
    "- After each epoch: plot eval predicted probability distribution with x-axis=log10(pred p).\n",
    "- After training: plot Train vs Eval BCE curves; print final BCE; collect eval predictions and plot eval charts analogous to input charts (N lines).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6afcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "from typing import Tuple, List, Optional\n",
    "import os\n",
    "os.environ[\"LOCALIZED_ENTROPY_NUM_WORKERS\"] = \"1\"\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "torch.set_printoptions(precision=4)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "NON_BLOCKING = USE_CUDA\n",
    "\n",
    "if USE_CUDA:\n",
    "    gpu_name = torch.cuda.get_device_name(device)\n",
    "    print(f'Using CUDA device: {gpu_name}')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    print('CUDA not available, defaulting to CPU.')\n",
    "\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14fb85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability generation\n",
    "def _sigmoid(x: np.ndarray, mu: float, s: float) -> np.ndarray:\n",
    "    return 1.0 / (1.0 + np.exp(-(x - mu) / s))\n",
    "\n",
    "def generate_probs(\n",
    "    num_samples: int,\n",
    "    mu_ln: float,\n",
    "    sigma_ln: float,\n",
    "    sig_mu: float,\n",
    "    sig_s: float,\n",
    "    mu_age: float,\n",
    "    sigma_age: float,\n",
    "    interestScale: float,\n",
    "    min_age: int = 10,\n",
    "    max_age: int = 100,\n",
    "    rng: Optional[np.random.Generator] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Implements generation logic and returns\n",
    "    (net_worth, ages, probabilities).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    net_worth = rng.lognormal(mean=mu_ln, sigma=sigma_ln, size=num_samples)\n",
    "    probs = _sigmoid(np.log10(net_worth + 1.0), mu=sig_mu, s=sig_s)\n",
    "\n",
    "    ages = rng.integers(min_age, max_age, size=num_samples)\n",
    "    denom = norm.pdf(50, loc=mu_age, scale=sigma_age)\n",
    "    denom = denom if denom > 0 else 1.0\n",
    "    interest_prob = norm.pdf(ages, loc=mu_age, scale=sigma_age) / denom\n",
    "    interest_prob = interest_prob / interestScale\n",
    "\n",
    "    probs = probs * interest_prob\n",
    "    probs = np.clip(probs, 0.0, 1.0)\n",
    "    return net_worth.astype(np.float32), ages.astype(np.float32), probs.astype(np.float32)\n",
    "\n",
    "def sample_condition_params(rng: np.random.Generator) -> Tuple[float, float, float, float, float, float, float]:\n",
    "    # Parameter ranges\n",
    "    mu_ln = 10.0\n",
    "    sigma_ln = 1.5\n",
    "    sig_mu = rng.choice(np.linspace(5, 7, 50))\n",
    "    sig_s = rng.choice(np.linspace(0.2, 0.3, 50))\n",
    "    mu_age = rng.choice(np.linspace(30, 70, 50))\n",
    "    sigma_age = rng.choice(np.linspace(10, 30, 50))\n",
    "    interestScale = 10.0 ** (rng.choice(np.linspace(0, 0.001, 50)))\n",
    "    return mu_ln, sigma_ln, sig_mu, sig_s, mu_age, sigma_age, interestScale\n",
    "\n",
    "def make_dataset(\n",
    "    num_conditions: int,\n",
    "    min_samples_per_condition: int = 90_000,\n",
    "    max_samples_per_condition: int = 100_000,\n",
    "    seed: int = 42,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns: ages, net_worth, condition_ids, labels, probs\n",
    "    Labels are sampled via Binomial(1, p)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    ages_all, nw_all, conds_all, labels_all, probs_all = [], [], [], [], []\n",
    "    for cond in range(num_conditions):\n",
    "        n = int(rng.integers(min_samples_per_condition, max_samples_per_condition + 1))\n",
    "        params = sample_condition_params(rng)\n",
    "        net_worth, ages, probs = generate_probs(n, *params, rng=rng)\n",
    "        labels = rng.binomial(n=1, p=probs).astype(np.float32)\n",
    "        ages_all.append(ages)\n",
    "        nw_all.append(net_worth)\n",
    "        conds_all.append(np.full_like(ages, fill_value=cond, dtype=np.float32))\n",
    "        labels_all.append(labels)\n",
    "        probs_all.append(probs)\n",
    "    ages = np.concatenate(ages_all, axis=0)\n",
    "    net_worth = np.concatenate(nw_all, axis=0)\n",
    "    conds = np.concatenate(conds_all, axis=0)\n",
    "    labels = np.concatenate(labels_all, axis=0)\n",
    "    probs = np.concatenate(probs_all, axis=0)\n",
    "    return ages, net_worth, conds.astype(np.int64), labels, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc094d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting helpers\n",
    "def _density_lines(\n",
    "    values: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    num_conditions: int,\n",
    "    *,\n",
    "    bins: int = 100,\n",
    "    transform: Optional[str] = None,  # None or 'log10'\n",
    "    value_range: Optional[Tuple[float, float]] = None,\n",
    "    title: str = '',\n",
    "    x_label: str = ''\n",
    ") -> None:\n",
    "    vals = values.astype(np.float64).copy()\n",
    "    if transform == 'log10':\n",
    "        eps = 1e-12\n",
    "        vals = np.log10(np.clip(vals, eps, None))\n",
    "    if value_range is None:\n",
    "        vmin, vmax = float(np.nanmin(vals)), float(np.nanmax(vals))\n",
    "    else:\n",
    "        vmin, vmax = value_range\n",
    "    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin == vmax:\n",
    "        vmin, vmax = 0.0, 1.0\n",
    "    edges = np.linspace(vmin, vmax, bins + 1)\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for cond in range(int(num_conditions)):\n",
    "        m = groups == cond\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        vv = vals[m]\n",
    "        hist, _ = np.histogram(vv, bins=edges, density=True)\n",
    "        plt.plot(centers, hist, label=f'Condition {cond}')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(ncol=2, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_distributions(net_worth: np.ndarray,\n",
    "                                ages: np.ndarray,\n",
    "                                probs: np.ndarray,\n",
    "                                conds: np.ndarray,\n",
    "                                num_conditions: int) -> None:\n",
    "    _density_lines(\n",
    "        values=net_worth,\n",
    "        groups=conds,\n",
    "        num_conditions=num_conditions,\n",
    "        bins=120,\n",
    "        transform='log10',\n",
    "        title='Training Data: Distribution by Condition (log10(NetWorth))',\n",
    "        x_label='log10(NetWorth)'\n",
    "    )\n",
    "    _density_lines(\n",
    "        values=ages,\n",
    "        groups=conds,\n",
    "        num_conditions=num_conditions,\n",
    "        bins=120,\n",
    "        transform=None,\n",
    "        title='Training Data: Distribution by Condition (Age)',\n",
    "        x_label='Age'\n",
    "    )\n",
    "    _density_lines(\n",
    "        values=probs,\n",
    "        groups=conds,\n",
    "        num_conditions=num_conditions,\n",
    "        bins=120,\n",
    "        transform='log10',\n",
    "        value_range=(-12, 0),\n",
    "        title='Training Data: Distribution by Condition (log10(true probability))',\n",
    "        x_label='log10(true p)'\n",
    "    )\n",
    "\n",
    "def plot_eval_log10p_hist(preds: np.ndarray, epoch: int, bins: int = 100) -> None:\n",
    "    eps = 1e-12\n",
    "    log10p = np.log10(np.clip(preds, eps, 1.0))\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(log10p, bins=bins, range=(-12, 0), density=True, color='#4477aa', alpha=0.85)\n",
    "    plt.title(f'Eval Predicted Probability: log10(p) (Epoch {epoch})')\n",
    "    plt.xlabel('log10(pred p)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db29a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset (numeric features + categorical condition)\n",
    "class MedCondDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_num: np.ndarray,\n",
    "        conds: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "        net_worth: Optional[np.ndarray] = None,\n",
    "        share_memory: bool = False,\n",
    "    ):\n",
    "        assert x_num.ndim == 2 and x_num.shape[1] == 2\n",
    "        assert len(x_num) == len(conds) == len(labels)\n",
    "        self.x = torch.as_tensor(x_num, dtype=torch.float32).contiguous()\n",
    "        self.c = torch.as_tensor(conds, dtype=torch.long).contiguous()\n",
    "        self.y = torch.as_tensor(labels, dtype=torch.float32).contiguous()\n",
    "        if net_worth is None:\n",
    "            self.nw = torch.zeros(len(labels), dtype=torch.float32)\n",
    "        else:\n",
    "            assert len(net_worth) == len(labels)\n",
    "            self.nw = torch.as_tensor(net_worth, dtype=torch.float32).contiguous()\n",
    "        if share_memory:\n",
    "            for tensor in (self.x, self.c, self.y, self.nw):\n",
    "                if tensor.device.type == 'cpu':\n",
    "                    tensor.share_memory_()\n",
    "    def __len__(self) -> int:\n",
    "        return self.y.numel()\n",
    "    def __getitem__(self, idx: int):\n",
    "        return (\n",
    "            self.x[idx],\n",
    "            self.c[idx],\n",
    "            self.y[idx],\n",
    "            self.nw[idx],\n",
    "        )\n",
    "\n",
    "class TensorBatchLoader:\n",
    "    def __init__(self, tensors: Tuple[torch.Tensor, ...], batch_size: int, shuffle: bool):\n",
    "        assert len(tensors) > 0\n",
    "        n = tensors[0].shape[0]\n",
    "        for t in tensors[1:]:\n",
    "            assert t.shape[0] == n, 'All tensors must share the first dimension.'\n",
    "        self.tensors = tensors\n",
    "        self.batch_size = int(max(1, batch_size))\n",
    "        self.shuffle = shuffle\n",
    "        self.length = n\n",
    "        self.device = tensors[0].device\n",
    "    def __len__(self) -> int:\n",
    "        return (self.length + self.batch_size - 1) // self.batch_size\n",
    "    @property\n",
    "    def num_workers(self) -> int:\n",
    "        return 0\n",
    "    def __iter__(self):\n",
    "        indices = torch.arange(self.length, device=self.device, dtype=torch.long)\n",
    "        if self.shuffle:\n",
    "            indices = indices[torch.randperm(self.length, device=self.device)]\n",
    "        for start in range(0, self.length, self.batch_size):\n",
    "            batch_idx = indices[start:start + self.batch_size]\n",
    "            yield tuple(t.index_select(0, batch_idx) for t in self.tensors)\n",
    "\n",
    "class ConditionProbNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_conditions: int,\n",
    "        embed_dim: int = 16,\n",
    "        hidden_sizes: Optional[Tuple[int, ...]] = None,\n",
    "        p_drop: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if hidden_sizes is None:\n",
    "            hidden_sizes = (256, 256, 128, 64)\n",
    "        self.embedding = nn.Embedding(num_conditions, embed_dim)\n",
    "        layers: List[nn.Module] = []\n",
    "        in_dim = embed_dim + 2\n",
    "        for hidden in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(in_dim, hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=p_drop),\n",
    "            ])\n",
    "            in_dim = hidden\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x_num: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n",
    "        emb = self.embedding(cond)\n",
    "        x = torch.cat([x_num, emb], dim=-1)\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "# Custom BCE loss (loop-based, with logits)\n",
    "def custom_bce_with_logits_loop(\n",
    "    logits: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    conditions_np: np.ndarray,\n",
    "    net_worth_np: np.ndarray,\n",
    "    condition_weights: Optional[np.ndarray] = None,\n",
    "    nw_threshold: Optional[float] = None,\n",
    "    nw_multiplier: float = 1.0,\n",
    "    reduction: str = 'mean'\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loop-based BCE-with-logits. Accepts an extra numpy array of\n",
    "    per-sample condition ids for future custom modifications.\n",
    "    \"\"\"\n",
    "    logits_flat = logits.view(-1)\n",
    "    targets_flat = targets.view(-1)\n",
    "    conds_flat = np.asarray(conditions_np).reshape(-1)\n",
    "    nw_flat = np.asarray(net_worth_np).reshape(-1)\n",
    "    total = torch.zeros((), device=logits.device, dtype=logits.dtype)\n",
    "    for i in range(logits_flat.shape[0]):\n",
    "        z = logits_flat[i]\n",
    "        y = targets_flat[i]\n",
    "        cond_id = int(conds_flat[i])  # available for custom logic\n",
    "        nw_val = float(nw_flat[i])    # raw net worth (for custom logic)\n",
    "        # Determine per-condition weight (default=1.0)\n",
    "        if condition_weights is not None:\n",
    "            try:\n",
    "                w = float(condition_weights[cond_id])\n",
    "                if not np.isfinite(w) or w <= 0:\n",
    "                    w = 1.0\n",
    "            except Exception:\n",
    "                w = 1.0\n",
    "        else:\n",
    "            w = 1.0\n",
    "        # Optional: upweight samples above a net-worth threshold\n",
    "        if (nw_threshold is not None) and (nw_multiplier != 1.0):\n",
    "            if np.isfinite(nw_val) and (nw_val >= nw_threshold):\n",
    "                w = w * float(nw_multiplier)\n",
    "        # Standard stable BCE-with-logits term, weighted\n",
    "        per_sample = (torch.clamp_min(z, 0.0) - z * y + torch.log1p(torch.exp(-torch.abs(z))))\n",
    "        total = total + (w * per_sample)\n",
    "    if reduction == 'mean':\n",
    "        return total / logits_flat.shape[0]\n",
    "    elif reduction == 'sum':\n",
    "        return total\n",
    "    else:\n",
    "        return total\n",
    "\n",
    "def localized_entropy_bce_torch(\n",
    "      logits: torch.Tensor,\n",
    "      targets: torch.Tensor,\n",
    "      conditions: torch.Tensor,\n",
    "      base_rates: Optional[torch.Tensor] = None,         # optional per-condition base rates p_j\n",
    "      net_worth: Optional[torch.Tensor] = None,          # unused currently\n",
    "      condition_weights: Optional[torch.Tensor] = None,  # optional per-condition weight\n",
    "      nw_threshold: Optional[float] = None,              # unused currently\n",
    "      nw_multiplier: float = 1.0,                        # unused currently\n",
    "      reduction: str = 'mean',\n",
    "      eps: float = 1e-12,\n",
    "  ) -> torch.Tensor:\n",
    "      \"\"\"\n",
    "      Localized Entropy (LE) implementation using PyTorch.\n",
    "\n",
    "      Mathematics\n",
    "      -----------\n",
    "        Let classes be indexed by j = 1..M, with N_j samples in class j,\n",
    "        labels y_i ∈ {0,1}, predicted probs ŷ_i = σ(z_i) from logits z_i, and\n",
    "        per-class base rate p_j = mean(y | class j).\n",
    "\n",
    "        Define cross-entropy over class j:\n",
    "          CE_j(y, q) = Σ_{i in class j} [ -y_i * log(q_i) - (1 - y_i) * log(1 - q_i) ]\n",
    "\n",
    "        Localized Entropy:\n",
    "          LE = ( Σ_{j=1..M}  CE_j(y, ŷ) / CE_j(y, p_j) ) / ( Σ_{j=1..M} N_j )\n",
    "\n",
    "        - Numerator uses per-sample predictions (stable BCE-with-logits).\n",
    "        - Denominator uses a constant predictor at the class base rate p_j.\n",
    "        - We clamp probabilities to [eps, 1-eps] for numerical stability.\n",
    "        - We divide by total samples Σ_j N_j (not by number of classes).\n",
    "\n",
    "      Gradient safety\n",
    "      ---------------\n",
    "        - All math is in torch; gradients flow through logits z_i in the numerator.\n",
    "        - Denominator depends on labels only (p_j from y), so it's treated as a\n",
    "          constant w.r.t. logits, which matches the intended LE definition.\n",
    "\n",
    "      Parameters\n",
    "      ----------\n",
    "        logits:        shape (N,) or (N,1). Raw model outputs z_i.\n",
    "        targets:       shape (N,). Binary labels {0,1}.\n",
    "        conditions:    shape (N,). Integer class IDs per sample.\n",
    "        base_rates:    Optional 1D tensor of length >= max condition id + 1.\n",
    "                       If provided, uses base_rates[j] as p_j for denominator.\n",
    "                       Expected on same device/dtype as logits/targets (or will be cast).\n",
    "        condition_weights:\n",
    "                       Optional 1D tensor of per-class weights where index is class id.\n",
    "                       If provided and indexable for a class id, scales that class term.\n",
    "        reduction:     'mean' (default) returns LE as defined; 'sum' returns LE * N.\n",
    "        eps:           Small constant for numerical stability.\n",
    "\n",
    "      Returns\n",
    "      -------\n",
    "        Scalar torch.Tensor (loss).\n",
    "      \"\"\"\n",
    "\n",
    "      # Flatten and align dtypes\n",
    "      z = logits.view(-1)                 # logits z_i\n",
    "      y = targets.view(-1).to(z.dtype)    # labels as float\n",
    "      c = conditions.view(-1).to(torch.long)\n",
    "\n",
    "      # Stable BCE-with-logits per-sample numerator terms\n",
    "      # BCE(z,y) = clamp_min(z,0) - z*y + log1p(exp(-|z|))\n",
    "      bce_per = torch.clamp_min(z, 0) - z * y + torch.log1p(torch.exp(-torch.abs(z)))\n",
    "\n",
    "      total = z.new_zeros(())             # accumulator for Σ_j (Num_j / Den_j)\n",
    "      unique_conds = torch.unique(c)\n",
    "      N = y.numel()                       # total number of samples (Σ_j N_j)\n",
    "\n",
    "      # Iterate classes j and compute normalized term\n",
    "      for cid in unique_conds:\n",
    "          mask = (c == cid)\n",
    "\n",
    "          # NUMERATOR: CE_j(y, ŷ) = Σ_i BCE(z_i, y_i) over class j\n",
    "          num = bce_per[mask].sum()\n",
    "\n",
    "          # DENOMINATOR: CE_j(y, p_j) with constant p_j = mean(y | class j)\n",
    "          yj = y[mask]\n",
    "          n = mask.sum()                    # N_j (int tensor)\n",
    "          ones = yj.sum()                   # Σ y_i (float)\n",
    "          zeros = n.to(y.dtype) - ones      # N_j - Σ y_i\n",
    "          # Determine p_j: prefer provided base_rates if available; else batch mean\n",
    "          if base_rates is not None:\n",
    "              idx = cid.item()\n",
    "              if 0 <= idx < base_rates.numel():\n",
    "                  pj = base_rates[idx].to(y.dtype)\n",
    "                  if not torch.isfinite(pj):\n",
    "                      pj = ones / n.clamp_min(1)\n",
    "              else:\n",
    "                  pj = ones / n.clamp_min(1)\n",
    "          else:\n",
    "              pj = ones / n.clamp_min(1)\n",
    "          pj = pj.clamp(eps, 1.0 - eps)     # p_j in [eps, 1-eps]\n",
    "\n",
    "          # CE at constant q_i = p_j:\n",
    "          # Σ_i [ -y_i*log(p_j) - (1 - y_i)*log(1 - p_j) ]\n",
    "          den = ones * (-torch.log(pj)) + zeros * (-torch.log1p(-pj))\n",
    "\n",
    "          # Normalized class contribution: CE_j(y, ŷ) / CE_j(y, p_j)\n",
    "          class_term = num / den.clamp_min(eps)\n",
    "\n",
    "          # Optional per-class weight w_j (default 1.0 if not provided or invalid)\n",
    "          if condition_weights is not None:\n",
    "              # Expect weights indexed by class id (0..K-1). If cid beyond length,\n",
    "              # or weight is non-finite/non-positive, fall back to 1.0.\n",
    "              idx = cid.item()\n",
    "              if 0 <= idx < condition_weights.numel():\n",
    "                  w = condition_weights[idx]\n",
    "                  if torch.isfinite(w) and (w > 0):\n",
    "                      class_term = class_term * w\n",
    "                  # else leave unweighted\n",
    "\n",
    "          total += class_term\n",
    "\n",
    "      # Aggregate across classes and average by total samples Σ_j N_j\n",
    "      loss = total / max(N, 1)\n",
    "\n",
    "      if reduction == 'sum':\n",
    "          return loss * N\n",
    "      return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa6898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    condition_weights: Optional[np.ndarray] = None,\n",
    "    nw_threshold: Optional[float] = None,\n",
    "    nw_multiplier: float = 1.0,\n",
    "    non_blocking: bool = False,\n",
    ") -> Tuple[float, np.ndarray]:\n",
    "    model.eval()\n",
    "    # Using custom BCE with logits (loop-based)\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    preds_all = []\n",
    "    verified_cuda_batch = False\n",
    "    for x, c, y, nw in loader:\n",
    "        x = x.to(device, non_blocking=non_blocking)\n",
    "        c = c.to(device, non_blocking=non_blocking)\n",
    "        y = y.to(device, non_blocking=non_blocking)\n",
    "        nw = nw.to(device, non_blocking=non_blocking)\n",
    "        logits = model(x, c)\n",
    "        if (device.type == 'cuda') and (not verified_cuda_batch):\n",
    "            tensors = (x, c, y, nw, logits)\n",
    "            if any(t.device.type != 'cuda' for t in tensors):\n",
    "                raise RuntimeError('Expected CUDA tensors during evaluation but found CPU tensors.')\n",
    "            verified_cuda_batch = True\n",
    "        loss = localized_entropy_bce_torch(\n",
    "            logits=logits,\n",
    "            targets=y,\n",
    "            conditions=c,\n",
    "            net_worth=nw,\n",
    "            condition_weights=(\n",
    "            torch.as_tensor(condition_weights, device=logits.device, dtype=logits.dtype)\n",
    "            if condition_weights is not None else None),\n",
    "            nw_threshold=nw_threshold,\n",
    "            nw_multiplier=nw_multiplier,\n",
    "        )\n",
    "        total_loss += float(loss.item()) * x.size(0)\n",
    "        total_count += x.size(0)\n",
    "        p = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        preds_all.append(p)\n",
    "    mean_loss = total_loss / max(1, total_count)\n",
    "    preds = np.concatenate(preds_all, axis=0)\n",
    "    return mean_loss, preds\n",
    "\n",
    "# Per-epoch cumulative base-rate tracker\n",
    "class StreamingBaseRate:\n",
    "    def __init__(self, num_conditions: int, device: torch.device, dtype: torch.dtype = torch.float32):\n",
    "        self.num_conditions = int(num_conditions)\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.counts = torch.zeros(self.num_conditions, dtype=torch.long, device=self.device)\n",
    "        self.sum_ones = torch.zeros(self.num_conditions, dtype=self.dtype, device=self.device)\n",
    "    @torch.no_grad()\n",
    "    def update(self, y: torch.Tensor, c: torch.Tensor):\n",
    "        c = c.view(-1).to(torch.long)\n",
    "        y = y.view(-1).to(self.dtype)\n",
    "        cnt = torch.bincount(c, minlength=self.num_conditions)\n",
    "        s1 = torch.bincount(c, weights=y, minlength=self.num_conditions)\n",
    "        self.counts += cnt\n",
    "        self.sum_ones += s1\n",
    "    @torch.no_grad()\n",
    "    def rates(self, eps: float = 1e-12) -> torch.Tensor:\n",
    "        denom = self.counts.clamp_min(1).to(self.sum_ones.dtype)\n",
    "        return (self.sum_ones / denom).clamp(eps, 1.0 - eps)\n",
    "\n",
    "def train_with_epoch_plots(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    condition_weights: Optional[np.ndarray] = None,\n",
    "    nw_threshold: Optional[float] = None,\n",
    "    nw_multiplier: float = 1.0,\n",
    "    non_blocking: bool = False,\n",
    "    plot_eval_hist_epochs: bool = False,\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # Using custom BCE with logits (loop-based)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    train_losses: List[float] = []\n",
    "    val_losses: List[float] = []\n",
    "    if device.type == 'cuda':\n",
    "        first_param = next(model.parameters(), None)\n",
    "        if (first_param is not None) and (first_param.device.type != 'cuda'):\n",
    "            raise RuntimeError('Model parameters must be on CUDA when device is CUDA.')\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        # Reset per-epoch cumulative base rates (class label means)\n",
    "        num_conds_model = getattr(model, 'embedding').num_embeddings if hasattr(model, 'embedding') else 1\n",
    "        first_param = next(model.parameters(), None)\n",
    "        p_dtype = first_param.dtype if first_param is not None else torch.float32\n",
    "        br_tracker = StreamingBaseRate(num_conds_model, device=device, dtype=p_dtype)\n",
    "        running = 0.0\n",
    "        count = 0\n",
    "        verified_cuda_batch = False\n",
    "        epoch_start = time.time()\n",
    "        for x, c, y, nw in train_loader:\n",
    "            x = x.to(device, non_blocking=non_blocking)\n",
    "            c = c.to(device, non_blocking=non_blocking)\n",
    "            y = y.to(device, non_blocking=non_blocking)\n",
    "            nw = nw.to(device, non_blocking=non_blocking)\n",
    "            if (device.type == 'cuda') and (not verified_cuda_batch):\n",
    "                tensors = (x, c, y, nw)\n",
    "                if any(t.device.type != 'cuda' for t in tensors):\n",
    "                    raise RuntimeError('Detected CPU tensors in the training loop while using CUDA.')\n",
    "                verified_cuda_batch = True\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x, c)\n",
    "            # Update per-epoch base rates with current batch, then use them for denominator\n",
    "            br_tracker.update(y, c)\n",
    "            loss = localized_entropy_bce_torch(\n",
    "                logits=logits,\n",
    "                targets=y,\n",
    "                conditions=c,\n",
    "                base_rates=br_tracker.rates(),\n",
    "                net_worth=nw,\n",
    "                condition_weights=(\n",
    "                torch.as_tensor(condition_weights, device=logits.device, dtype=logits.dtype)\n",
    "                if condition_weights is not None else None),\n",
    "                nw_threshold=nw_threshold,\n",
    "                nw_multiplier=nw_multiplier,\n",
    "            )\n",
    "            # loss = loss_fn(logits, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running += float(loss.item()) * x.size(0)\n",
    "            count += x.size(0)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize(device)\n",
    "        train_bce = running / max(1, count)\n",
    "        val_bce, preds = evaluate(\n",
    "            model, val_loader, device,\n",
    "            condition_weights=condition_weights,\n",
    "            nw_threshold=nw_threshold,\n",
    "            nw_multiplier=nw_multiplier,\n",
    "            non_blocking=non_blocking,\n",
    "        )\n",
    "        train_losses.append(train_bce)\n",
    "        val_losses.append(val_bce)\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        log_msg = f'Epoch {epoch:3d}/{epochs} | Train BCE: {train_bce:.6f} | Eval BCE: {val_bce:.6f} | wall {epoch_time:.2f}s'\n",
    "        if device.type == 'cuda':\n",
    "            mem_alloc = torch.cuda.memory_allocated(device) / 1e6\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / 1e6\n",
    "            log_msg += f\" | cuda_mem={mem_alloc:.1f}MB (peak {peak_mem:.1f}MB)\"\n",
    "        print(log_msg)\n",
    "    if plot_eval_hist_epochs:\n",
    "        plot_eval_log10p_hist(preds.astype(np.float32), epoch)\n",
    "    print(f'Final Train BCE: {train_losses[-1]:.10f}')\n",
    "    print(f'Final Eval  BCE: {val_losses[-1]:.10f}')\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Configuration & data generation\n",
    "seed = 42\n",
    "epochs = 12\n",
    "lr = 1e-3\n",
    "batch_size = 25_000\n",
    "num_conditions = 10\n",
    "min_samples_per_condition = 200_000\n",
    "max_samples_per_condition = 200_000\n",
    "PLOT_DATA_BEFORE_TRAINING = False\n",
    "PLOT_DATA_AFTER_TRAINING = True\n",
    "PLOT_EVAL_HIST_EPOCHS = False\n",
    "MOVE_DATASET_TO_CUDA = True\n",
    "model_hidden_sizes = (256, 256, 128, 64)\n",
    "model_embed_dim = 16\n",
    "model_dropout = 0.3\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "print('Generating dataset...')\n",
    "ages, net_worth, conds, labels, probs = make_dataset(\n",
    "    num_conditions=num_conditions,\n",
    "    min_samples_per_condition=min_samples_per_condition,\n",
    "    max_samples_per_condition=max_samples_per_condition,\n",
    "    seed=seed,\n",
    ")\n",
    "n_total = len(labels)\n",
    "print(f'Total samples: {n_total:,}')\n",
    "\n",
    "# Split 90/10 into train/eval\n",
    "idx = np.arange(n_total)\n",
    "np.random.shuffle(idx)\n",
    "split = int(0.9 * n_total)\n",
    "train_idx = idx[:split]\n",
    "eval_idx = idx[split:]\n",
    "\n",
    "age_tr, age_ev = ages[train_idx], ages[eval_idx]\n",
    "nw_tr, nw_ev = net_worth[train_idx], net_worth[eval_idx]\n",
    "cond_tr, cond_ev = conds[train_idx], conds[eval_idx]\n",
    "y_tr, y_ev = labels[train_idx], labels[eval_idx]\n",
    "p_tr, p_ev = probs[train_idx], probs[eval_idx]\n",
    "\n",
    "if PLOT_DATA_BEFORE_TRAINING:\n",
    "    plot_training_distributions(nw_tr, age_tr, p_tr, cond_tr, num_conditions)\n",
    "else:\n",
    "    print('Skipping training data distribution plots before training.')\n",
    "\n",
    "# Build numeric features: [Age, log10(NetWorth)] then standardize from training stats\n",
    "log10_nw_tr = np.log10(np.clip(nw_tr, 1e-12, None))\n",
    "log10_nw_ev = np.log10(np.clip(nw_ev, 1e-12, None))\n",
    "xnum_tr = np.stack([age_tr, log10_nw_tr], axis=1)\n",
    "xnum_ev = np.stack([age_ev, log10_nw_ev], axis=1)\n",
    "mu = xnum_tr.mean(axis=0)\n",
    "sd = xnum_tr.std(axis=0)\n",
    "sd[sd < 1e-6] = 1.0\n",
    "xnum_tr_n = (xnum_tr - mu) / sd\n",
    "xnum_ev_n = (xnum_ev - mu) / sd\n",
    "\n",
    "use_tensor_loader = USE_CUDA and MOVE_DATASET_TO_CUDA\n",
    "if use_tensor_loader:\n",
    "    print('Staging datasets directly on CUDA for batch sampling.')\n",
    "    train_tensors = (\n",
    "        torch.as_tensor(xnum_tr_n, dtype=torch.float32, device=device),\n",
    "        torch.as_tensor(cond_tr, dtype=torch.long, device=device),\n",
    "        torch.as_tensor(y_tr, dtype=torch.float32, device=device),\n",
    "        torch.as_tensor(nw_tr, dtype=torch.float32, device=device),\n",
    "    )\n",
    "    eval_tensors = (\n",
    "        torch.as_tensor(xnum_ev_n, dtype=torch.float32, device=device),\n",
    "        torch.as_tensor(cond_ev, dtype=torch.long, device=device),\n",
    "        torch.as_tensor(y_ev, dtype=torch.float32, device=device),\n",
    "        torch.as_tensor(nw_ev, dtype=torch.float32, device=device),\n",
    "    )\n",
    "    train_loader = TensorBatchLoader(train_tensors, batch_size=batch_size, shuffle=True)\n",
    "    eval_loader = TensorBatchLoader(eval_tensors, batch_size=batch_size, shuffle=False)\n",
    "    loader_note = (\n",
    "        f'TensorBatchLoader on CUDA (batches per epoch: {len(train_loader)} / {len(eval_loader)}).'\n",
    "    )\n",
    "else:\n",
    "    train_ds = MedCondDataset(xnum_tr_n, cond_tr, y_tr, net_worth=nw_tr)\n",
    "    eval_ds = MedCondDataset(xnum_ev_n, cond_ev, y_ev, net_worth=nw_ev)\n",
    "    loader_common = dict(batch_size=batch_size, drop_last=False, pin_memory=USE_CUDA)\n",
    "    max_workers = os.cpu_count() or 1\n",
    "    env_override = os.environ.get('LOCALIZED_ENTROPY_NUM_WORKERS')\n",
    "    if env_override is not None:\n",
    "        try:\n",
    "            num_workers = max(0, min(int(env_override), max_workers))\n",
    "        except ValueError:\n",
    "            num_workers = 0\n",
    "    else:\n",
    "        num_workers = 0 if USE_CUDA else min(2, max_workers)\n",
    "    worker_kwargs = {}\n",
    "    if num_workers > 0:\n",
    "        worker_kwargs = dict(num_workers=num_workers, persistent_workers=False, prefetch_factor=2)\n",
    "\n",
    "    def _instantiate_loader(dataset: Dataset, *, shuffle: bool, use_workers: bool) -> DataLoader:\n",
    "        kwargs = dict(loader_common)\n",
    "        if use_workers and worker_kwargs:\n",
    "            kwargs.update(worker_kwargs)\n",
    "        kwargs['shuffle'] = shuffle\n",
    "        return DataLoader(dataset, **kwargs)\n",
    "\n",
    "    def _build_loader(dataset: Dataset, *, shuffle: bool, role: str) -> DataLoader:\n",
    "        if not worker_kwargs:\n",
    "            return _instantiate_loader(dataset, shuffle=shuffle, use_workers=False)\n",
    "        test_iter = None\n",
    "        try:\n",
    "            test_loader = _instantiate_loader(dataset, shuffle=shuffle, use_workers=True)\n",
    "            test_iter = iter(test_loader)\n",
    "            next(test_iter)\n",
    "        except Exception as exc:\n",
    "            if test_iter is not None and hasattr(test_iter, '_shutdown_workers'):\n",
    "                test_iter._shutdown_workers()\n",
    "            print(f\"[WARN] {role} DataLoader workers failed ({exc}); falling back to workers=0.\")\n",
    "            return _instantiate_loader(dataset, shuffle=shuffle, use_workers=False)\n",
    "        else:\n",
    "            if test_iter is not None and hasattr(test_iter, '_shutdown_workers'):\n",
    "                test_iter._shutdown_workers()\n",
    "            return _instantiate_loader(dataset, shuffle=shuffle, use_workers=True)\n",
    "\n",
    "    train_loader = _build_loader(train_ds, shuffle=True, role='Train')\n",
    "    eval_loader = _build_loader(eval_ds, shuffle=False, role='Eval')\n",
    "    train_workers = getattr(train_loader, 'num_workers', 0)\n",
    "    eval_workers = getattr(eval_loader, 'num_workers', 0)\n",
    "    loader_note = (\n",
    "        f\"Train/Eval DataLoader workers: {train_workers}/{eval_workers} (pin_memory={loader_common.get('pin_memory', False)})\"\n",
    "    )\n",
    "    if USE_CUDA and train_workers == 0:\n",
    "        loader_note += \" | Multiprocessing disabled for CUDA stability; set LOCALIZED_ENTROPY_NUM_WORKERS>0 to retry.\"\n",
    "\n",
    "print(loader_note)\n",
    "\n",
    "model = ConditionProbNet(\n",
    "    num_conditions=num_conditions,\n",
    "    embed_dim=model_embed_dim,\n",
    "    hidden_sizes=model_hidden_sizes,\n",
    "    p_drop=model_dropout,\n",
    ").to(device)\n",
    "model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train\n",
    "train_losses, eval_losses = train_with_epoch_plots(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=eval_loader,\n",
    "    device=device,\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    "    non_blocking=NON_BLOCKING,\n",
    "    plot_eval_hist_epochs=PLOT_EVAL_HIST_EPOCHS,\n",
    ")\n",
    "train_losses, eval_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4942c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot BCE curves\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label='Train BCE')\n",
    "plt.plot(eval_losses, label='Eval BCE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BCE Loss')\n",
    "plt.title('Training vs Evaluation BCE over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Final Train BCE: {train_losses[-1]:.10f}')\n",
    "print(f'Final Eval  BCE: {eval_losses[-1]:.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e165b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final evaluation and plots analogous to input\n",
    "eval_bce, eval_preds = evaluate(model, eval_loader, device, non_blocking=NON_BLOCKING)\n",
    "eval_probs = np.clip(eval_preds, 1e-12, 1 - 1e-12)\n",
    "plot_eval_log10p_hist(eval_preds.astype(np.float32), epoch=epochs)\n",
    "\n",
    "# Distribution plots for evaluation predictions vs conditions\n",
    "_density_lines(\n",
    "    values=eval_preds,\n",
    "    groups=cond_ev,\n",
    "    num_conditions=num_conditions,\n",
    "    bins=120,\n",
    "    transform='log10',\n",
    "    value_range=(-12, 0),\n",
    "    title='Eval Predictions: Distribution by Condition (log10(pred probability))',\n",
    "    x_label='log10(pred p)'\n",
    ")\n",
    "\n",
    "print(f'Final Evaluation BCE: {eval_bce:.10f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d4421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: render training data distributions after training\n",
    "if PLOT_DATA_AFTER_TRAINING:\n",
    "    plot_training_distributions(nw_tr, age_tr, p_tr, cond_tr, num_conditions)\n",
    "else:\n",
    "    print('Post-training training data plots are disabled. Set PLOT_DATA_AFTER_TRAINING=True to enable.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "le-stats-utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LE per-condition stats + plotting\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def collect_le_stats_per_condition_torch(logits: torch.Tensor,\n",
    "                                        targets: torch.Tensor,\n",
    "                                        conditions: torch.Tensor,\n",
    "                                        eps: float = 1e-12):\n",
    "    \"\"\"Compute LE numerator/denominator terms per condition using logits.\n",
    "    Returns a dict: cond_id -> metrics.\n",
    "    \"\"\"\n",
    "    z = logits.view(-1)\n",
    "    y = targets.view(-1).to(z.dtype)\n",
    "    c = conditions.view(-1).to(torch.long)\n",
    "\n",
    "    # Stable BCE-with-logits per-sample\n",
    "    bce_per = torch.clamp_min(z, 0) - z * y + torch.log1p(torch.exp(-torch.abs(z)))\n",
    "\n",
    "    stats = {}\n",
    "    for cid in torch.unique(c):\n",
    "        mask = (c == cid)\n",
    "        n = int(mask.sum().item())\n",
    "        if n == 0:\n",
    "            continue\n",
    "        yj = y[mask]\n",
    "        num = float(bce_per[mask].sum().item())\n",
    "        ones = float(yj.sum().item())\n",
    "        zeros = float(n) - ones\n",
    "        pj = ones / max(1.0, float(n))\n",
    "        pj = max(eps, min(1.0 - eps, pj))\n",
    "        den = ones * (-math.log(pj)) + zeros * (-math.log1p(-pj))\n",
    "        ratio = num / (den if den > eps else eps)\n",
    "        stats[int(cid.item())] = {\n",
    "            'Numerator': num,\n",
    "            'Denominator': den,\n",
    "            'Average prediction for denominator': pj,\n",
    "            'Number of samples with label 1': int(round(ones)),\n",
    "            'Number of samples with label 0': int(round(zeros)),\n",
    "            'Numerator/denominator': ratio,\n",
    "        }\n",
    "    return stats\n",
    "\n",
    "def plot_le_stats_per_condition(stats: dict, title: str = 'Localized Entropy terms per condition'):\n",
    "    conds = sorted(stats.keys())\n",
    "    nums = [stats[c]['Numerator'] for c in conds]\n",
    "    dens = [stats[c]['Denominator'] for c in conds]\n",
    "    ratios = [stats[c]['Numerator/denominator'] for c in conds]\n",
    "    pj = [stats[c]['Average prediction for denominator'] for c in conds]\n",
    "    n1 = [stats[c]['Number of samples with label 1'] for c in conds]\n",
    "    n0 = [stats[c]['Number of samples with label 0'] for c in conds]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "    # Ratio\n",
    "    axs[0, 0].bar(conds, ratios, color='#4477aa')\n",
    "    axs[0, 0].set_title('Numerator / Denominator')\n",
    "    axs[0, 0].set_xlabel('Condition')\n",
    "    axs[0, 0].set_ylabel('Ratio')\n",
    "    axs[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Numerator vs Denominator\n",
    "    x = np.arange(len(conds))\n",
    "    width = 0.4\n",
    "    axs[0, 1].bar(x - width/2, nums, width=width, label='Numerator', color='#66c2a5')\n",
    "    axs[0, 1].bar(x + width/2, dens, width=width, label='Denominator', color='#fc8d62')\n",
    "    axs[0, 1].set_xticks(x)\n",
    "    axs[0, 1].set_xticklabels(conds)\n",
    "    axs[0, 1].set_title('Numerator vs Denominator')\n",
    "    axs[0, 1].legend()\n",
    "    axs[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Counts\n",
    "    axs[1, 0].bar(conds, n0, label='Label 0 count', color='#999999')\n",
    "    axs[1, 0].bar(conds, n1, bottom=n0, label='Label 1 count', color='#1b9e77')\n",
    "    axs[1, 0].set_title('Label counts per condition')\n",
    "    axs[1, 0].set_xlabel('Condition')\n",
    "    axs[1, 0].set_ylabel('Count')\n",
    "    axs[1, 0].legend()\n",
    "    axs[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Base rate pj\n",
    "    axs[1, 1].bar(conds, pj, color='#8da0cb')\n",
    "    axs[1, 1].set_title('Average prediction for denominator (p_j)')\n",
    "    axs[1, 1].set_xlabel('Condition')\n",
    "    axs[1, 1].set_ylabel('p_j')\n",
    "    axs[1, 1].set_ylim(0, 1)\n",
    "    axs[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "le-stats-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect & plot LE stats per condition on eval set (using logits)\n",
    "model.eval()\n",
    "all_logits, all_targets, all_conditions = [], [], []\n",
    "with torch.no_grad():\n",
    "    for xb, cb, yb, nw in eval_loader:\n",
    "        xb = xb.to(device, non_blocking=NON_BLOCKING)\n",
    "        cb = cb.to(device, non_blocking=NON_BLOCKING)\n",
    "        yb = yb.to(device, non_blocking=NON_BLOCKING)\n",
    "        nw = nw.to(device, non_blocking=NON_BLOCKING)\n",
    "        zb = model(xb, cb)\n",
    "        all_logits.append(zb.detach().cpu())\n",
    "        all_targets.append(yb.detach().cpu())\n",
    "        all_conditions.append(cb.detach().cpu())\n",
    "z_all = torch.cat(all_logits).view(-1)\n",
    "y_all = torch.cat(all_targets).view(-1)\n",
    "c_all = torch.cat(all_conditions).view(-1)\n",
    "\n",
    "le_stats = collect_le_stats_per_condition_torch(z_all, y_all, c_all, eps=1e-12)\n",
    "\n",
    "# Print table\n",
    "print('cond\tnum\tden\tavg_p\t#y=1\t#y=0\tratio')\n",
    "for cond in sorted(le_stats.keys()):\n",
    "    s = le_stats[cond]\n",
    "    print(f\"{cond}\t{s['Numerator']:.6g}\t{s['Denominator']:.6g}\t{s['Average prediction for denominator']:.6g}\t{s['Number of samples with label 1']}\t{s['Number of samples with label 0']}\t{s['Numerator/denominator']:.6g}\")\n",
    "\n",
    "plot_le_stats_per_condition(le_stats, title='Localized Entropy terms per condition - Eval set')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
