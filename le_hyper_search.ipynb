{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LE Hyperparameter Search (`lr` / epochs)\n",
        "\n",
        "This notebook runs a Localized Entropy (LE) hyperparameter search using `configs/default.json` as-is.\n",
        "\n",
        "- Outer loop: learning rate (`lr`) starts at `1.0` and halves each step.\n",
        "- Inner loop: train for `10` epochs and evaluate each epoch.\n",
        "- Per-epoch collection:\n",
        "  - LE loss on test/eval split\n",
        "  - Global calibration ratio on all labeled data\n",
        "  - Per-condition calibration ratio on all labeled data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from localized_entropy.analysis import per_condition_calibration\n",
        "from localized_entropy.config import load_and_resolve, get_data_source\n",
        "from localized_entropy.data.pipeline import prepare_data\n",
        "from localized_entropy.experiments import build_loss_loaders, build_model, train_single_loss\n",
        "from localized_entropy.training import predict_probs, evaluate, compute_base_rates_from_loader\n",
        "from localized_entropy.utils import init_device, set_seed\n",
        "\n",
        "np.set_printoptions(precision=6, suppress=True)\n",
        "pd.set_option(\"display.max_rows\", 200)\n",
        "pd.set_option(\"display.max_columns\", 50)\n",
        "\n",
        "CONFIG_PATH = \"configs/default.json\"\n",
        "cfg = load_and_resolve(CONFIG_PATH)\n",
        "\n",
        "device_cfg = cfg.get(\"device\", {})\n",
        "device, use_cuda, use_mps, non_blocking = init_device(use_mps=bool(device_cfg.get(\"use_mps\", True)))\n",
        "cpu_float64 = device.type == \"cpu\" and not bool(device_cfg.get(\"use_mps\", True))\n",
        "model_dtype = torch.float64 if cpu_float64 else torch.float32\n",
        "\n",
        "set_seed(int(cfg[\"project\"][\"seed\"]), use_cuda)\n",
        "prepared = prepare_data(cfg, device, use_cuda, use_mps)\n",
        "splits = prepared.splits\n",
        "\n",
        "loss_loaders, le_train_cfg = build_loss_loaders(cfg, \"localized_entropy\", splits, device, use_cuda, use_mps)\n",
        "data_source = get_data_source(cfg)\n",
        "\n",
        "test_has_labels = not (data_source == \"ctr\" and not bool(cfg.get(\"ctr\", {}).get(\"test_has_labels\", False)))\n",
        "if loss_loaders.test_loader is not None and splits.y_test is not None and test_has_labels:\n",
        "    eval_loader = loss_loaders.test_loader\n",
        "    eval_labels = np.asarray(splits.y_test).reshape(-1)\n",
        "    eval_conds = np.asarray(splits.c_test).reshape(-1)\n",
        "    eval_name = \"test\"\n",
        "else:\n",
        "    eval_loader = loss_loaders.eval_loader\n",
        "    eval_labels = np.asarray(splits.y_eval).reshape(-1)\n",
        "    eval_conds = np.asarray(splits.c_eval).reshape(-1)\n",
        "    eval_name = \"eval\"\n",
        "\n",
        "print(f\"Evaluation split for LE loss: {eval_name}\")\n",
        "print(f\"Data source: {data_source}\")\n",
        "print(f\"Num conditions/categories: {splits.num_conditions}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lr_schedule_halving(start: float = 1.0, floor: float = 1e-6):\n",
        "    values = []\n",
        "    lr = float(start)\n",
        "    while lr >= floor:\n",
        "        values.append(lr)\n",
        "        lr = lr / 2.0\n",
        "    return values\n",
        "\n",
        "\n",
        "def lr_category_schedule_decade(start: float = 1.0, floor: float = 1e-6):\n",
        "    values = []\n",
        "    lr_category = float(start)\n",
        "    while lr_category >= floor:\n",
        "        values.append(lr_category)\n",
        "        lr_category = lr_category / 10.0\n",
        "    return values\n",
        "\n",
        "\n",
        "def _global_calibration_ratio(preds: np.ndarray, labels: np.ndarray, eps: float = 1e-12) -> float:\n",
        "    pred_mean = float(np.mean(preds))\n",
        "    label_mean = float(np.mean(labels))\n",
        "    if label_mean <= eps:\n",
        "        return float(\"nan\")\n",
        "    return pred_mean / label_mean\n",
        "\n",
        "\n",
        "def collect_all_data_metrics(model: torch.nn.Module) -> tuple[float, pd.DataFrame]:\n",
        "    train_preds = predict_probs(model, loss_loaders.train_loader, device, non_blocking=non_blocking)\n",
        "    eval_preds_local = predict_probs(model, loss_loaders.eval_loader, device, non_blocking=non_blocking)\n",
        "\n",
        "    preds_parts = [train_preds.reshape(-1), eval_preds_local.reshape(-1)]\n",
        "    labels_parts = [np.asarray(splits.y_train).reshape(-1), np.asarray(splits.y_eval).reshape(-1)]\n",
        "    conds_parts = [np.asarray(splits.c_train).reshape(-1), np.asarray(splits.c_eval).reshape(-1)]\n",
        "\n",
        "    if loss_loaders.test_loader is not None and splits.y_test is not None and test_has_labels:\n",
        "        test_preds_local = predict_probs(model, loss_loaders.test_loader, device, non_blocking=non_blocking)\n",
        "        preds_parts.append(test_preds_local.reshape(-1))\n",
        "        labels_parts.append(np.asarray(splits.y_test).reshape(-1))\n",
        "        conds_parts.append(np.asarray(splits.c_test).reshape(-1))\n",
        "\n",
        "    all_preds = np.concatenate(preds_parts)\n",
        "    all_labels = np.concatenate(labels_parts)\n",
        "    all_conds = np.concatenate(conds_parts)\n",
        "\n",
        "    global_calibration = _global_calibration_ratio(all_preds, all_labels)\n",
        "    per_cond = per_condition_calibration(all_preds, all_labels, all_conds)\n",
        "    return global_calibration, per_cond\n",
        "\n",
        "\n",
        "LEARNING_RATES = lr_schedule_halving(start=1.0, floor=1e-6)\n",
        "LR_CATEGORY_RATES = lr_category_schedule_decade(start=1.0, floor=1e-6)\n",
        "EPOCHS = 15\n",
        "\n",
        "print(f\"Number of lr points: {len(LEARNING_RATES)}\")\n",
        "print(f\"Number of lr_category points: {len(LR_CATEGORY_RATES)}\")\n",
        "print(\"lr first/last:\", LEARNING_RATES[0], LEARNING_RATES[-1])\n",
        "print(\"lr_category first/last:\", LR_CATEGORY_RATES[0], LR_CATEGORY_RATES[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoch_records = []\n",
        "condition_records = []\n",
        "\n",
        "first_param_dtype = torch.float32\n",
        "if cpu_float64:\n",
        "    first_param_dtype = torch.float64\n",
        "\n",
        "# LE evaluation needs base rates from training data.\n",
        "base_rates_train = compute_base_rates_from_loader(\n",
        "    loss_loaders.train_loader,\n",
        "    num_conditions=int(splits.num_conditions),\n",
        "    device=device,\n",
        "    dtype=first_param_dtype,\n",
        "    non_blocking=non_blocking,\n",
        ")\n",
        "\n",
        "for lr_category in LR_CATEGORY_RATES:\n",
        "    for lr in LEARNING_RATES:\n",
        "        set_seed(int(cfg[\"project\"][\"seed\"]), use_cuda)\n",
        "        model = build_model(cfg, splits, device, dtype=model_dtype)\n",
        "\n",
        "        def on_epoch_eval(_eval_preds: np.ndarray, epoch: int, lr_value: float = lr, lr_category_value: float = lr_category) -> None:\n",
        "            le_loss, _ = evaluate(\n",
        "                model,\n",
        "                eval_loader,\n",
        "                device,\n",
        "                loss_mode=\"localized_entropy\",\n",
        "                base_rates=base_rates_train,\n",
        "                non_blocking=non_blocking,\n",
        "            )\n",
        "            global_calibration, per_cond_df = collect_all_data_metrics(model)\n",
        "\n",
        "            epoch_records.append(\n",
        "                {\n",
        "                    \"lr_category\": float(lr_category_value),\n",
        "                    \"lr\": float(lr_value),\n",
        "                    \"epoch\": int(epoch),\n",
        "                    \"test_le\": float(le_loss),\n",
        "                    \"global_calibration\": float(global_calibration),\n",
        "                }\n",
        "            )\n",
        "\n",
        "            for _, row in per_cond_df.iterrows():\n",
        "                condition_records.append(\n",
        "                    {\n",
        "                        \"lr_category\": float(lr_category_value),\n",
        "                        \"lr\": float(lr_value),\n",
        "                        \"epoch\": int(epoch),\n",
        "                        \"condition\": int(row[\"condition\"]),\n",
        "                        \"count\": int(row[\"count\"]),\n",
        "                        \"base_rate\": float(row[\"base_rate\"]),\n",
        "                        \"pred_mean\": float(row[\"pred_mean\"]),\n",
        "                        \"calibration\": float(row[\"calibration\"]),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        _ = train_single_loss(\n",
        "            model=model,\n",
        "            loss_mode=\"localized_entropy\",\n",
        "            train_loader=loss_loaders.train_loader,\n",
        "            train_eval_loader=eval_loader,\n",
        "            eval_loader=eval_loader,\n",
        "            device=device,\n",
        "            epochs=EPOCHS,\n",
        "            lr=float(lr),\n",
        "            lr_decay=float(le_train_cfg.get('lr_decay', cfg['training'].get('lr_decay', 1.0))),\n",
        "            lr_category_decay=float(le_train_cfg.get('lr_category_decay', cfg['training'].get('lr_category_decay', 1.0))),\n",
        "            lr_category=float(lr_category),\n",
        "            eval_has_labels=True,\n",
        "            le_base_rates_train=base_rates_train,\n",
        "            le_base_rates_train_eval=base_rates_train,\n",
        "            le_base_rates_eval=base_rates_train,\n",
        "            non_blocking=non_blocking,\n",
        "            eval_callback=on_epoch_eval,\n",
        "            plot_eval_hist_epochs=False,\n",
        "            print_embedding_table=False,\n",
        "        )\n",
        "\n",
        "results_df = pd.DataFrame(epoch_records)\n",
        "condition_df = pd.DataFrame(condition_records)\n",
        "\n",
        "if results_df.empty:\n",
        "    raise RuntimeError(\"Search produced no results.\")\n",
        "\n",
        "results_df = results_df.sort_values([\"test_le\", \"lr_category\", \"lr\", \"epoch\"], ascending=[True, True, True, True]).reset_index(drop=True)\n",
        "best_row = results_df.iloc[0]\n",
        "\n",
        "print(\"Best LE result overall:\")\n",
        "print(best_row.to_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_metric_lines(metric_df: pd.DataFrame, value_col: str, title: str, y_label: str) -> None:\n",
        "    plt.figure(figsize=(11, 6))\n",
        "    lr_order = sorted(metric_df[\"lr\"].unique(), reverse=True)\n",
        "    for lr in lr_order:\n",
        "        block = metric_df.loc[metric_df[\"lr\"] == lr, [\"epoch\", value_col]].sort_values(\"epoch\")\n",
        "        plt.plot(\n",
        "            block[\"epoch\"],\n",
        "            block[value_col],\n",
        "            marker=\"o\",\n",
        "            linewidth=1.5,\n",
        "            markersize=3,\n",
        "            label=f\"lr={lr:.6g}\",\n",
        "        )\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(y_label)\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(ncol=3, fontsize=8)\n",
        "    if \"calibration\" in value_col.lower():\n",
        "        plt.ylim(0, 3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "table_cols = [\"lr_category\", \"lr\", \"epoch\", \"test_le\", \"global_calibration\"]\n",
        "lr_category_order = sorted(results_df[\"lr_category\"].unique(), reverse=True)\n",
        "\n",
        "for lr_category in lr_category_order:\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"lr_category = {lr_category:.6g}\")\n",
        "\n",
        "    cat_results = results_df.loc[results_df[\"lr_category\"] == lr_category].copy()\n",
        "    cat_results = cat_results.sort_values([\"test_le\", \"lr\", \"epoch\"], ascending=[True, True, True]).reset_index(drop=True)\n",
        "    cat_condition = condition_df.loc[condition_df[\"lr_category\"] == lr_category].copy()\n",
        "\n",
        "    display(cat_results.loc[:, table_cols].head(50))\n",
        "\n",
        "    best_row_cat = cat_results.iloc[0]\n",
        "    print(\"Most optimal lr (by minimum test_le):\", float(best_row_cat[\"lr\"]))\n",
        "    print(\"Best epoch for that lr:\", int(best_row_cat[\"epoch\"]))\n",
        "    print(\"Best test_le:\", float(best_row_cat[\"test_le\"]))\n",
        "\n",
        "    # Chart 1: LE across learning rate and epochs\n",
        "    plot_metric_lines(\n",
        "        metric_df=cat_results,\n",
        "        value_col=\"test_le\",\n",
        "        title=f\"LE ({eval_name}) across learning rate and epoch | lr_category={lr_category:.6g}\",\n",
        "        y_label=\"LE loss\",\n",
        "    )\n",
        "\n",
        "    # Chart 2: Global calibration across learning rate and epochs\n",
        "    plot_metric_lines(\n",
        "        metric_df=cat_results,\n",
        "        value_col=\"global_calibration\",\n",
        "        title=f\"Global calibration ratio across learning rate and epoch | lr_category={lr_category:.6g}\",\n",
        "        y_label=\"Calibration ratio\",\n",
        "    )\n",
        "\n",
        "    # Chart 3: Per-condition calibration charts (line per lr)\n",
        "    # Per-condition calibration table used for charts (this lr_category)\n",
        "    condition_table_cols = [\"lr_category\", \"lr\", \"epoch\", \"condition\", \"count\", \"base_rate\", \"pred_mean\", \"calibration\"]\n",
        "    display(cat_condition.loc[:, condition_table_cols].sort_values([\"condition\", \"lr\", \"epoch\"]).reset_index(drop=True))\n",
        "\n",
        "    if cat_condition.empty:\n",
        "        print(\"No per-condition records to plot.\")\n",
        "        continue\n",
        "\n",
        "    cond_ids = sorted(cat_condition[\"condition\"].unique())\n",
        "    n = len(cond_ids)\n",
        "    ncols = 3\n",
        "    nrows = int(math.ceil(n / ncols))\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5 * ncols, 3.8 * nrows))\n",
        "    axes = np.array(axes).reshape(-1)\n",
        "\n",
        "    lr_order = sorted(cat_condition[\"lr\"].unique(), reverse=True)\n",
        "    epoch_order = sorted(cat_condition[\"epoch\"].unique())\n",
        "\n",
        "    for i, cond_id in enumerate(cond_ids):\n",
        "        ax = axes[i]\n",
        "        block = cat_condition.loc[cat_condition[\"condition\"] == cond_id, [\"lr\", \"epoch\", \"calibration\"]]\n",
        "        ax.set_title(f\"Condition {cond_id}\")\n",
        "        for lr in lr_order:\n",
        "            lr_block = block.loc[block[\"lr\"] == lr, [\"epoch\", \"calibration\"]].sort_values(\"epoch\")\n",
        "            if lr_block.empty:\n",
        "                continue\n",
        "            ax.plot(\n",
        "                lr_block[\"epoch\"],\n",
        "                lr_block[\"calibration\"],\n",
        "                marker=\"o\",\n",
        "                linewidth=1.2,\n",
        "                markersize=2.5,\n",
        "                label=f\"{lr:.3g}\",\n",
        "            )\n",
        "        ax.set_xticks(epoch_order)\n",
        "        ax.set_xlabel(\"Epoch\")\n",
        "        ax.set_ylabel(\"Calibration\")\n",
        "        ax.set_ylim(0, 3)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.legend(fontsize=6)\n",
        "\n",
        "    for j in range(len(cond_ids), len(axes)):\n",
        "        axes[j].axis(\"off\")\n",
        "\n",
        "    fig.suptitle(f\"Per-condition calibration ratio across epoch (line per lr) | lr_category={lr_category:.6g}\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
